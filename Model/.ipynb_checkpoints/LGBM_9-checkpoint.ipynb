{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arkarmin/anaconda3/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from datetime import date, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib as plp\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\n",
    "    '../Data/train_set_one_year.csv', usecols=[1, 2, 3, 4, 5],\n",
    "    converters={'unit_sales': lambda u: np.log1p(\n",
    "        float(u)) if float(u) > 0 else 0},\n",
    "    parse_dates=[\"date\"],dtype={'onpromotion': bool}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>onpromotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23808256</th>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>54</td>\n",
       "      <td>2089339</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23808257</th>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>54</td>\n",
       "      <td>2106464</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23808258</th>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>54</td>\n",
       "      <td>2110456</td>\n",
       "      <td>5.262690</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23808259</th>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>54</td>\n",
       "      <td>2113914</td>\n",
       "      <td>5.293305</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23808260</th>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>54</td>\n",
       "      <td>2116416</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               date  store_nbr  item_nbr  unit_sales  onpromotion\n",
       "23808256 2017-08-15         54   2089339    1.609438        False\n",
       "23808257 2017-08-15         54   2106464    0.693147         True\n",
       "23808258 2017-08-15         54   2110456    5.262690        False\n",
       "23808259 2017-08-15         54   2113914    5.293305         True\n",
       "23808260 2017-08-15         54   2116416    1.098612        False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\n",
    "    \"../Data/test.csv\", usecols=[0, 1, 2, 3, 4],\n",
    "    dtype={'onpromotion': bool},\n",
    "    parse_dates=[\"date\"]  # , date_parser=parser\n",
    ").set_index(\n",
    "    ['store_nbr', 'item_nbr', 'date']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(\n",
    "    \"../Data/items.csv\",\n",
    ").set_index(\"item_nbr\") #In order to give weight to item perishable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_train = df_train.set_index(\n",
    "    [\"store_nbr\", \"item_nbr\", \"date\"])[[\"onpromotion\"]].unstack(\n",
    "        level=-1).fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_train.columns = promo_train.columns.get_level_values(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_test = df_test[[\"onpromotion\"]].unstack(level=-1).fillna(False)\n",
    "promo_test.columns = promo_test.columns.get_level_values(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_test = promo_test.reindex(promo_train.index).fillna(False)\n",
    "promo_2017 = pd.concat([promo_train, promo_test], axis=1)\n",
    "del promo_test, promo_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.set_index(\n",
    "    [\"store_nbr\", \"item_nbr\", \"date\"])[[\"unit_sales\"]].unstack(\n",
    "        level=-1).fillna(0)\n",
    "df_train.columns = df_train.columns.get_level_values(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = items.reindex(df_train.index.get_level_values(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timespan(df, dt, minus, periods, freq='D'):\n",
    "    return df[pd.date_range(dt - timedelta(days=minus), periods=periods, freq=freq)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_dataset(t2017, is_train=True):\n",
    "#     X = pd.DataFrame({\n",
    "#         \"day_1_2017\": get_timespan(df_train, t2017, 1, 1).values.ravel(),\n",
    "#         \"mean_3_2017\": get_timespan(df_train, t2017, 3, 3).mean(axis=1).values,\n",
    "#         \"mean_7_2017\": get_timespan(df_train, t2017, 7, 7).mean(axis=1).values,\n",
    "#         \"mean_14_2017\": get_timespan(df_train, t2017, 14, 14).mean(axis=1).values,\n",
    "#         \"mean_16_2017\": get_timespan(df_train, t2017, 16, 16).mean(axis=1).values,\n",
    "#         \"mean_30_2017\": get_timespan(df_train, t2017, 30, 30).mean(axis=1).values,\n",
    "#         \"median_3_2017\": get_timespan(df_train, t2017, 3, 3).median(axis=1).values,\n",
    "#         \"median_7_2017\": get_timespan(df_train, t2017, 7, 7).median(axis=1).values,\n",
    "#         \"median_14_2017\": get_timespan(df_train, t2017, 14, 14).median(axis=1).values,\n",
    "#         \"median_16_2017\": get_timespan(df_train, t2017, 16, 16).median(axis=1).values,\n",
    "#         \"median_30_2017\": get_timespan(df_train, t2017, 30, 30).median(axis=1).values,\n",
    "#         \"std_3_2017\": get_timespan(df_train, t2017, 3, 3).std(axis=1).values,\n",
    "#         \"std_7_2017\": get_timespan(df_train, t2017, 7, 7).std(axis=1).values,\n",
    "#         \"std_14_2017\": get_timespan(df_train, t2017, 14, 14).std(axis=1).values,\n",
    "#         \"std_16_2017\": get_timespan(df_train, t2017, 16, 16).std(axis=1).values,\n",
    "#         \"std_30_2017\": get_timespan(df_train, t2017, 30, 30).std(axis=1).values,\n",
    "#         \"promo_14_2017\": get_timespan(promo_2017, t2017, 14, 14).sum(axis=1).values,\n",
    "#         \"promo_30_2017\": get_timespan(promo_2017, t2017, 30, 30).sum(axis=1).values \n",
    "#     })\n",
    "#     for i in range(7):\n",
    "#         X['mean_4_dow{}_2017'.format(i)] = get_timespan(df_train, t2017, 28-i, 4, freq='7D').mean(axis=1).values\n",
    "#     for i in range(16):\n",
    "#         X[\"promo_{}\".format(i)] = promo_2017[t2017 + timedelta(days=i)].values.astype(np.uint8)\n",
    "#     if is_train:\n",
    "#         y = df_train[\n",
    "#             pd.date_range(t2017, periods=16)\n",
    "#         ].values\n",
    "#         return X, y\n",
    "#     return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(t2017, is_train=True):\n",
    "    X = pd.DataFrame({\n",
    "          \"day_1_2017\": get_timespan(df_train, t2017, 1, 1).values.ravel()\n",
    "        })\n",
    "    for i in [3,7,14,16,21,30,60,140]:\n",
    "            X[\"mean_\"+str(i)] = get_timespan(df_train, t2017, i, i).mean(axis=1).values\n",
    "            X[\"median_\"+str(i)] = get_timespan(df_train, t2017, i, i).median(axis=1).values\n",
    "            X[\"std_\"+str(i)] = get_timespan(df_train, t2017, i, i).std(axis=1).values\n",
    "            X[\"promo_\"+str(i)] = get_timespan(promo_2017, t2017, i, i).sum(axis=1).values\n",
    "            X[\"max_\"+str(i)] = get_timespan(promo_2017, t2017, i, i).max(axis=1).values\n",
    "            X[\"min_\"+str(i)] = get_timespan(promo_2017, t2017, i, i).min(axis=1).values\n",
    "    for i in range(7):\n",
    "        X['mean_4_dow{}_2017'.format(i)] = get_timespan(df_train, t2017, 28-i, 4, freq='7D').mean(axis=1).values\n",
    "        X['mean_20_dow{}_2017'.format(i)] = get_timespan(df_train, t2017, 140-i, 20, freq='7D').mean(axis=1).values\n",
    "    for i in range(16):\n",
    "        X[\"promo_{}\".format(i)] = promo_2017[t2017 + timedelta(days=i)].values.astype(np.uint8)\n",
    "    if is_train:\n",
    "        y = df_train[\n",
    "            pd.date_range(t2017, periods=16)\n",
    "        ].values\n",
    "        return X, y\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing dataset...\")\n",
    "t2017 = date(2017, 6, 21)\n",
    "X_l, y_l = [], []\n",
    "for i in range(4):\n",
    "    delta = timedelta(days=7 * i)\n",
    "    X_tmp, y_tmp = prepare_dataset(t2017 + delta)\n",
    "    X_l.append(X_tmp)\n",
    "    y_l.append(y_tmp)\n",
    "X_train = pd.concat(X_l, axis=0)\n",
    "y_train = np.concatenate(y_l, axis=0)\n",
    "del X_l, y_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = prepare_dataset(date(2017, 7, 26))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = prepare_dataset(date(2017, 8, 16), is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting models...\n",
      "==================================================\n",
      "Step 1\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's l2: 0.314318\tvalid_1's l2: 0.309892\n",
      "[200]\ttraining's l2: 0.291895\tvalid_1's l2: 0.288639\n",
      "[300]\ttraining's l2: 0.2879\tvalid_1's l2: 0.284772\n",
      "[400]\ttraining's l2: 0.285527\tvalid_1's l2: 0.282451\n",
      "[500]\ttraining's l2: 0.283773\tvalid_1's l2: 0.280664\n",
      "[600]\ttraining's l2: 0.282196\tvalid_1's l2: 0.279068\n",
      "[700]\ttraining's l2: 0.280817\tvalid_1's l2: 0.277705\n",
      "[800]\ttraining's l2: 0.279469\tvalid_1's l2: 0.276358\n",
      "[900]\ttraining's l2: 0.278185\tvalid_1's l2: 0.275073\n",
      "[1000]\ttraining's l2: 0.276955\tvalid_1's l2: 0.273864\n",
      "[1100]\ttraining's l2: 0.275771\tvalid_1's l2: 0.272699\n",
      "[1200]\ttraining's l2: 0.274636\tvalid_1's l2: 0.271556\n",
      "[1300]\ttraining's l2: 0.273518\tvalid_1's l2: 0.270498\n",
      "[1400]\ttraining's l2: 0.27243\tvalid_1's l2: 0.269416\n",
      "[1500]\ttraining's l2: 0.27139\tvalid_1's l2: 0.268396\n",
      "[1600]\ttraining's l2: 0.270391\tvalid_1's l2: 0.267393\n",
      "[1700]\ttraining's l2: 0.269406\tvalid_1's l2: 0.2664\n",
      "[1800]\ttraining's l2: 0.268404\tvalid_1's l2: 0.26542\n",
      "[1900]\ttraining's l2: 0.267425\tvalid_1's l2: 0.264466\n",
      "[2000]\ttraining's l2: 0.26646\tvalid_1's l2: 0.263505\n",
      "[2100]\ttraining's l2: 0.265512\tvalid_1's l2: 0.262597\n",
      "[2200]\ttraining's l2: 0.264608\tvalid_1's l2: 0.261698\n",
      "[2300]\ttraining's l2: 0.263703\tvalid_1's l2: 0.260812\n",
      "[2400]\ttraining's l2: 0.262801\tvalid_1's l2: 0.259895\n",
      "[2500]\ttraining's l2: 0.261935\tvalid_1's l2: 0.259053\n",
      "[2600]\ttraining's l2: 0.261063\tvalid_1's l2: 0.258183\n",
      "[2700]\ttraining's l2: 0.260199\tvalid_1's l2: 0.257348\n",
      "[2800]\ttraining's l2: 0.259356\tvalid_1's l2: 0.256522\n",
      "[2900]\ttraining's l2: 0.258511\tvalid_1's l2: 0.255688\n",
      "[3000]\ttraining's l2: 0.257655\tvalid_1's l2: 0.254844\n",
      "[3100]\ttraining's l2: 0.256845\tvalid_1's l2: 0.254051\n",
      "[3200]\ttraining's l2: 0.256039\tvalid_1's l2: 0.253239\n",
      "[3300]\ttraining's l2: 0.255239\tvalid_1's l2: 0.252451\n",
      "[3400]\ttraining's l2: 0.254441\tvalid_1's l2: 0.251651\n",
      "[3500]\ttraining's l2: 0.253656\tvalid_1's l2: 0.250887\n",
      "[3600]\ttraining's l2: 0.252863\tvalid_1's l2: 0.250114\n",
      "[3700]\ttraining's l2: 0.252096\tvalid_1's l2: 0.249344\n",
      "[3800]\ttraining's l2: 0.25135\tvalid_1's l2: 0.248618\n",
      "[3900]\ttraining's l2: 0.250609\tvalid_1's l2: 0.247896\n",
      "[4000]\ttraining's l2: 0.249848\tvalid_1's l2: 0.24716\n",
      "[4100]\ttraining's l2: 0.249101\tvalid_1's l2: 0.246417\n",
      "[4200]\ttraining's l2: 0.248358\tvalid_1's l2: 0.245687\n",
      "[4300]\ttraining's l2: 0.247611\tvalid_1's l2: 0.244944\n",
      "[4400]\ttraining's l2: 0.246897\tvalid_1's l2: 0.244239\n",
      "[4500]\ttraining's l2: 0.246181\tvalid_1's l2: 0.243532\n",
      "[4600]\ttraining's l2: 0.245466\tvalid_1's l2: 0.242835\n",
      "[4700]\ttraining's l2: 0.24475\tvalid_1's l2: 0.242136\n",
      "[4800]\ttraining's l2: 0.244041\tvalid_1's l2: 0.241439\n",
      "[4900]\ttraining's l2: 0.243334\tvalid_1's l2: 0.240757\n",
      "[5000]\ttraining's l2: 0.242634\tvalid_1's l2: 0.240068\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l2: 0.242634\tvalid_1's l2: 0.240068\n",
      "mean_7: 7203225.96\n",
      "mean_14: 5268526.71\n",
      "mean_16: 619314.88\n",
      "promo_0: 439180.70\n",
      "median_7: 345396.93\n",
      "mean_20_dow0_2017: 294831.50\n",
      "mean_4_dow0_2017: 208734.03\n",
      "day_1_2017: 190267.66\n",
      "mean_3: 126512.32\n",
      "mean_60: 122255.78\n",
      "mean_21: 109007.05\n",
      "mean_30: 95079.58\n",
      "std_7: 72294.05\n",
      "std_140: 67271.39\n",
      "median_16: 65553.25\n",
      "promo_16: 56269.91\n",
      "std_3: 54192.39\n",
      "mean_4_dow5_2017: 52142.06\n",
      "mean_20_dow4_2017: 50276.72\n",
      "median_14: 49215.50\n",
      "std_16: 48619.13\n",
      "std_14: 48150.94\n",
      "mean_4_dow2_2017: 47626.58\n",
      "promo_7: 47452.75\n",
      "median_3: 46702.79\n",
      "std_60: 46689.02\n",
      "mean_4_dow6_2017: 45799.83\n",
      "median_60: 44085.79\n",
      "mean_20_dow2_2017: 43268.65\n",
      "std_30: 43085.60\n",
      "std_21: 43028.58\n",
      "mean_4_dow4_2017: 42402.22\n",
      "mean_4_dow1_2017: 41314.65\n",
      "mean_140: 41231.44\n",
      "mean_4_dow3_2017: 40985.93\n",
      "mean_20_dow6_2017: 38485.55\n",
      "promo_140: 38173.39\n",
      "mean_20_dow3_2017: 37142.05\n",
      "min_3: 37132.85\n",
      "mean_20_dow1_2017: 35745.07\n",
      "mean_20_dow5_2017: 34151.92\n",
      "median_30: 32203.02\n",
      "promo_60: 22552.98\n",
      "promo_30: 20634.51\n",
      "median_21: 19281.43\n",
      "max_3: 19238.12\n",
      "promo_21: 18765.50\n",
      "median_140: 16783.55\n",
      "promo_14: 16588.78\n",
      "max_7: 13283.88\n",
      "promo_15: 8756.70\n",
      "min_7: 8189.45\n",
      "promo_9: 7194.10\n",
      "max_14: 6091.63\n",
      "promo_13: 5128.41\n",
      "promo_2: 4939.71\n",
      "promo_3: 4252.45\n",
      "promo_11: 3819.92\n",
      "promo_1: 3151.90\n",
      "promo_4: 3030.39\n",
      "promo_12: 2069.03\n",
      "max_30: 1968.88\n",
      "promo_5: 1955.99\n",
      "max_140: 1854.50\n",
      "promo_6: 1502.59\n",
      "promo_8: 1489.33\n",
      "max_60: 1439.33\n",
      "promo_10: 1229.18\n",
      "max_16: 1066.48\n",
      "max_21: 424.87\n",
      "min_14: 111.35\n",
      "min_21: 52.33\n",
      "min_16: 14.32\n",
      "min_30: 4.34\n",
      "min_60: 0.00\n",
      "min_140: 0.00\n",
      "==================================================\n",
      "Step 2\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's l2: 0.333742\tvalid_1's l2: 0.339184\n",
      "[200]\ttraining's l2: 0.3147\tvalid_1's l2: 0.31879\n",
      "[300]\ttraining's l2: 0.310768\tvalid_1's l2: 0.313926\n",
      "[400]\ttraining's l2: 0.308256\tvalid_1's l2: 0.311006\n",
      "[500]\ttraining's l2: 0.306382\tvalid_1's l2: 0.308984\n",
      "[600]\ttraining's l2: 0.304782\tvalid_1's l2: 0.307316\n",
      "[700]\ttraining's l2: 0.303251\tvalid_1's l2: 0.305726\n",
      "[800]\ttraining's l2: 0.301803\tvalid_1's l2: 0.304247\n",
      "[900]\ttraining's l2: 0.300408\tvalid_1's l2: 0.302824\n",
      "[1000]\ttraining's l2: 0.299074\tvalid_1's l2: 0.301445\n",
      "[1100]\ttraining's l2: 0.297807\tvalid_1's l2: 0.300144\n",
      "[1200]\ttraining's l2: 0.296542\tvalid_1's l2: 0.298858\n",
      "[1300]\ttraining's l2: 0.295296\tvalid_1's l2: 0.297584\n",
      "[1400]\ttraining's l2: 0.294103\tvalid_1's l2: 0.296386\n",
      "[1500]\ttraining's l2: 0.292948\tvalid_1's l2: 0.295192\n",
      "[1600]\ttraining's l2: 0.291832\tvalid_1's l2: 0.294062\n",
      "[1700]\ttraining's l2: 0.290701\tvalid_1's l2: 0.292931\n",
      "[1800]\ttraining's l2: 0.289634\tvalid_1's l2: 0.29185\n",
      "[1900]\ttraining's l2: 0.288564\tvalid_1's l2: 0.29076\n",
      "[2000]\ttraining's l2: 0.287543\tvalid_1's l2: 0.289738\n",
      "[2100]\ttraining's l2: 0.28651\tvalid_1's l2: 0.288655\n",
      "[2200]\ttraining's l2: 0.28549\tvalid_1's l2: 0.2876\n",
      "[2300]\ttraining's l2: 0.284488\tvalid_1's l2: 0.286602\n",
      "[2400]\ttraining's l2: 0.283506\tvalid_1's l2: 0.28562\n",
      "[2500]\ttraining's l2: 0.282546\tvalid_1's l2: 0.284657\n",
      "[2600]\ttraining's l2: 0.281593\tvalid_1's l2: 0.283697\n",
      "[2700]\ttraining's l2: 0.280637\tvalid_1's l2: 0.282731\n",
      "[2800]\ttraining's l2: 0.279698\tvalid_1's l2: 0.281795\n",
      "[2900]\ttraining's l2: 0.278806\tvalid_1's l2: 0.280902\n",
      "[3000]\ttraining's l2: 0.277887\tvalid_1's l2: 0.27997\n",
      "[3100]\ttraining's l2: 0.277016\tvalid_1's l2: 0.279081\n",
      "[3200]\ttraining's l2: 0.276134\tvalid_1's l2: 0.278208\n",
      "[3300]\ttraining's l2: 0.275266\tvalid_1's l2: 0.277352\n",
      "[3400]\ttraining's l2: 0.274416\tvalid_1's l2: 0.276499\n",
      "[3500]\ttraining's l2: 0.273542\tvalid_1's l2: 0.275608\n",
      "[3600]\ttraining's l2: 0.272699\tvalid_1's l2: 0.274767\n",
      "[3700]\ttraining's l2: 0.271863\tvalid_1's l2: 0.273928\n",
      "[3800]\ttraining's l2: 0.271057\tvalid_1's l2: 0.273116\n",
      "[3900]\ttraining's l2: 0.270253\tvalid_1's l2: 0.272319\n",
      "[4000]\ttraining's l2: 0.269424\tvalid_1's l2: 0.271489\n",
      "[4100]\ttraining's l2: 0.268615\tvalid_1's l2: 0.270687\n",
      "[4200]\ttraining's l2: 0.267819\tvalid_1's l2: 0.269897\n",
      "[4300]\ttraining's l2: 0.26702\tvalid_1's l2: 0.269089\n",
      "[4400]\ttraining's l2: 0.266253\tvalid_1's l2: 0.268342\n",
      "[4500]\ttraining's l2: 0.265464\tvalid_1's l2: 0.267542\n",
      "[4600]\ttraining's l2: 0.264687\tvalid_1's l2: 0.266745\n",
      "[4700]\ttraining's l2: 0.263917\tvalid_1's l2: 0.265961\n",
      "[4800]\ttraining's l2: 0.26316\tvalid_1's l2: 0.265208\n",
      "[4900]\ttraining's l2: 0.26241\tvalid_1's l2: 0.264469\n",
      "[5000]\ttraining's l2: 0.261643\tvalid_1's l2: 0.263705\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l2: 0.261643\tvalid_1's l2: 0.263705\n",
      "mean_14: 6063780.44\n",
      "mean_7: 4386084.36\n",
      "mean_16: 409453.70\n",
      "promo_1: 323715.44\n",
      "median_7: 317861.31\n",
      "mean_20_dow1_2017: 248169.28\n",
      "mean_21: 215795.81\n",
      "mean_30: 211786.84\n",
      "median_60: 148487.04\n",
      "mean_4_dow1_2017: 112674.18\n",
      "mean_60: 96795.38\n",
      "mean_3: 88396.87\n",
      "day_1_2017: 75837.49\n",
      "std_7: 67404.28\n",
      "std_140: 66750.02\n",
      "promo_16: 60731.87\n",
      "median_30: 60120.74\n",
      "mean_20_dow2_2017: 57247.64\n",
      "mean_4_dow6_2017: 55549.45\n",
      "std_60: 53974.42\n",
      "std_14: 53776.16\n",
      "mean_20_dow4_2017: 52842.36\n",
      "mean_4_dow2_2017: 51642.37\n",
      "std_3: 51043.77\n",
      "mean_4_dow4_2017: 49891.45\n",
      "std_16: 47100.31\n",
      "mean_4_dow3_2017: 46693.18\n",
      "mean_4_dow0_2017: 46639.34\n",
      "mean_4_dow5_2017: 45900.67\n",
      "std_30: 45456.63\n",
      "std_21: 44386.87\n",
      "mean_20_dow0_2017: 41566.45\n",
      "median_14: 41529.31\n",
      "mean_20_dow6_2017: 41329.47\n",
      "mean_140: 41122.49\n",
      "mean_20_dow5_2017: 38512.36\n",
      "promo_140: 36444.43\n",
      "mean_20_dow3_2017: 36327.08\n",
      "median_16: 33170.18\n",
      "median_3: 33028.96\n",
      "promo_3: 32509.30\n",
      "promo_21: 28077.92\n",
      "min_3: 24998.50\n",
      "promo_60: 24129.89\n",
      "median_140: 23578.83\n",
      "promo_0: 23237.09\n",
      "promo_30: 20495.55\n",
      "median_21: 16180.35\n",
      "promo_5: 14102.25\n",
      "promo_4: 13605.61\n",
      "promo_2: 8367.51\n",
      "max_3: 7503.89\n",
      "min_7: 6487.28\n",
      "promo_7: 5213.76\n",
      "promo_6: 4868.44\n",
      "max_7: 4450.90\n",
      "promo_8: 3513.35\n",
      "promo_14: 3432.44\n",
      "promo_9: 2600.19\n",
      "max_14: 2375.58\n",
      "promo_13: 2132.90\n",
      "promo_15: 1906.91\n",
      "promo_11: 1746.15\n",
      "max_16: 1402.22\n",
      "max_140: 955.22\n",
      "promo_10: 935.78\n",
      "promo_12: 804.11\n",
      "max_30: 804.00\n",
      "max_60: 617.45\n",
      "max_21: 491.26\n",
      "min_16: 203.74\n",
      "min_14: 107.20\n",
      "min_21: 9.74\n",
      "min_60: 5.68\n",
      "min_30: 2.27\n",
      "min_140: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Step 3\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's l2: 0.343473\tvalid_1's l2: 0.35966\n",
      "[200]\ttraining's l2: 0.320316\tvalid_1's l2: 0.33394\n",
      "[300]\ttraining's l2: 0.315477\tvalid_1's l2: 0.328492\n",
      "[400]\ttraining's l2: 0.312567\tvalid_1's l2: 0.325309\n",
      "[500]\ttraining's l2: 0.310516\tvalid_1's l2: 0.32312\n",
      "[600]\ttraining's l2: 0.308716\tvalid_1's l2: 0.321213\n",
      "[700]\ttraining's l2: 0.307051\tvalid_1's l2: 0.319464\n",
      "[800]\ttraining's l2: 0.305484\tvalid_1's l2: 0.317765\n",
      "[900]\ttraining's l2: 0.304028\tvalid_1's l2: 0.316223\n",
      "[1000]\ttraining's l2: 0.302611\tvalid_1's l2: 0.314713\n",
      "[1100]\ttraining's l2: 0.301278\tvalid_1's l2: 0.31329\n",
      "[1200]\ttraining's l2: 0.299958\tvalid_1's l2: 0.311915\n",
      "[1300]\ttraining's l2: 0.298697\tvalid_1's l2: 0.310575\n",
      "[1400]\ttraining's l2: 0.29748\tvalid_1's l2: 0.309303\n",
      "[1500]\ttraining's l2: 0.296272\tvalid_1's l2: 0.308016\n",
      "[1600]\ttraining's l2: 0.295115\tvalid_1's l2: 0.306807\n",
      "[1700]\ttraining's l2: 0.293983\tvalid_1's l2: 0.305618\n",
      "[1800]\ttraining's l2: 0.292883\tvalid_1's l2: 0.304459\n",
      "[1900]\ttraining's l2: 0.29181\tvalid_1's l2: 0.303336\n",
      "[2000]\ttraining's l2: 0.29072\tvalid_1's l2: 0.302188\n",
      "[2100]\ttraining's l2: 0.289693\tvalid_1's l2: 0.301127\n",
      "[2200]\ttraining's l2: 0.288643\tvalid_1's l2: 0.300017\n",
      "[2300]\ttraining's l2: 0.287616\tvalid_1's l2: 0.298955\n",
      "[2400]\ttraining's l2: 0.286624\tvalid_1's l2: 0.297913\n",
      "[2500]\ttraining's l2: 0.285625\tvalid_1's l2: 0.296879\n",
      "[2600]\ttraining's l2: 0.28468\tvalid_1's l2: 0.295881\n",
      "[2700]\ttraining's l2: 0.283728\tvalid_1's l2: 0.294899\n",
      "[2800]\ttraining's l2: 0.282781\tvalid_1's l2: 0.293926\n",
      "[2900]\ttraining's l2: 0.281865\tvalid_1's l2: 0.292967\n",
      "[3000]\ttraining's l2: 0.280936\tvalid_1's l2: 0.29199\n",
      "[3100]\ttraining's l2: 0.280034\tvalid_1's l2: 0.291011\n",
      "[3200]\ttraining's l2: 0.279125\tvalid_1's l2: 0.290059\n",
      "[3300]\ttraining's l2: 0.27822\tvalid_1's l2: 0.289132\n",
      "[3400]\ttraining's l2: 0.277346\tvalid_1's l2: 0.288224\n",
      "[3500]\ttraining's l2: 0.276471\tvalid_1's l2: 0.287303\n",
      "[3600]\ttraining's l2: 0.275632\tvalid_1's l2: 0.286429\n",
      "[3700]\ttraining's l2: 0.274805\tvalid_1's l2: 0.285571\n",
      "[3800]\ttraining's l2: 0.273974\tvalid_1's l2: 0.284713\n",
      "[3900]\ttraining's l2: 0.273149\tvalid_1's l2: 0.283812\n",
      "[4000]\ttraining's l2: 0.272331\tvalid_1's l2: 0.282974\n",
      "[4100]\ttraining's l2: 0.271499\tvalid_1's l2: 0.282107\n",
      "[4200]\ttraining's l2: 0.270705\tvalid_1's l2: 0.281267\n",
      "[4300]\ttraining's l2: 0.26992\tvalid_1's l2: 0.28044\n",
      "[4400]\ttraining's l2: 0.269137\tvalid_1's l2: 0.279604\n",
      "[4500]\ttraining's l2: 0.268334\tvalid_1's l2: 0.278779\n",
      "[4600]\ttraining's l2: 0.267552\tvalid_1's l2: 0.277978\n",
      "[4700]\ttraining's l2: 0.266792\tvalid_1's l2: 0.277181\n",
      "[4800]\ttraining's l2: 0.266019\tvalid_1's l2: 0.27638\n",
      "[4900]\ttraining's l2: 0.265265\tvalid_1's l2: 0.275597\n",
      "[5000]\ttraining's l2: 0.264507\tvalid_1's l2: 0.274791\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l2: 0.264507\tvalid_1's l2: 0.274791\n",
      "mean_14: 6639897.96\n",
      "mean_7: 4767721.01\n",
      "mean_20_dow2_2017: 759668.57\n",
      "mean_4_dow2_2017: 754115.85\n",
      "mean_21: 485889.50\n",
      "promo_2: 410339.43\n",
      "mean_30: 409412.64\n",
      "median_7: 224663.77\n",
      "std_140: 89254.66\n",
      "mean_60: 79214.11\n",
      "mean_3: 77325.57\n",
      "std_7: 69975.76\n",
      "mean_16: 63742.24\n",
      "std_60: 62052.60\n",
      "median_60: 60525.83\n",
      "mean_20_dow4_2017: 58994.43\n",
      "day_1_2017: 58571.85\n",
      "promo_16: 56441.96\n",
      "mean_4_dow1_2017: 55814.03\n",
      "promo_140: 55226.26\n",
      "std_14: 54983.89\n",
      "mean_4_dow3_2017: 54659.41\n",
      "std_30: 54413.20\n",
      "mean_20_dow1_2017: 50320.97\n",
      "std_21: 49793.00\n",
      "mean_4_dow4_2017: 48255.46\n",
      "std_3: 48122.50\n",
      "std_16: 47730.80\n",
      "mean_4_dow0_2017: 47458.50\n",
      "mean_4_dow6_2017: 45902.62\n",
      "mean_4_dow5_2017: 44959.73\n",
      "mean_20_dow5_2017: 42423.64\n",
      "mean_20_dow6_2017: 41425.38\n",
      "mean_20_dow0_2017: 40481.43\n",
      "mean_20_dow3_2017: 39158.21\n",
      "promo_30: 33778.37\n",
      "promo_3: 32807.00\n",
      "mean_140: 32023.84\n",
      "promo_21: 31252.88\n",
      "promo_9: 31067.55\n",
      "median_3: 30959.29\n",
      "promo_60: 30406.59\n",
      "median_14: 27695.64\n",
      "median_30: 25802.97\n",
      "median_16: 23147.93\n",
      "promo_0: 21887.19\n",
      "median_140: 21363.57\n",
      "min_3: 18965.16\n",
      "promo_1: 15596.82\n",
      "median_21: 15281.70\n",
      "promo_5: 13654.31\n",
      "promo_4: 13190.64\n",
      "promo_7: 10770.45\n",
      "max_7: 9749.36\n",
      "promo_14: 8994.79\n",
      "max_3: 7616.33\n",
      "max_14: 6818.20\n",
      "promo_10: 6805.39\n",
      "promo_11: 6095.28\n",
      "promo_13: 5294.82\n",
      "promo_6: 4938.08\n",
      "promo_15: 4681.87\n",
      "promo_8: 3489.79\n",
      "promo_12: 2430.34\n",
      "max_30: 1769.60\n",
      "min_7: 1657.03\n",
      "max_16: 1643.50\n",
      "max_140: 1631.48\n",
      "max_21: 1336.11\n",
      "max_60: 1039.55\n",
      "min_14: 166.99\n",
      "min_21: 33.09\n",
      "min_16: 22.60\n",
      "min_30: 4.64\n",
      "min_60: 0.00\n",
      "min_140: 0.00\n",
      "==================================================\n",
      "Step 4\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's l2: 0.367512\tvalid_1's l2: 0.372128\n",
      "[200]\ttraining's l2: 0.343462\tvalid_1's l2: 0.347207\n",
      "[300]\ttraining's l2: 0.338405\tvalid_1's l2: 0.34166\n",
      "[400]\ttraining's l2: 0.335046\tvalid_1's l2: 0.338222\n",
      "[500]\ttraining's l2: 0.332701\tvalid_1's l2: 0.33585\n",
      "[600]\ttraining's l2: 0.330699\tvalid_1's l2: 0.333843\n",
      "[700]\ttraining's l2: 0.328883\tvalid_1's l2: 0.332011\n",
      "[800]\ttraining's l2: 0.327214\tvalid_1's l2: 0.330323\n",
      "[900]\ttraining's l2: 0.325663\tvalid_1's l2: 0.328757\n",
      "[1000]\ttraining's l2: 0.324147\tvalid_1's l2: 0.327241\n",
      "[1100]\ttraining's l2: 0.322713\tvalid_1's l2: 0.325781\n",
      "[1200]\ttraining's l2: 0.321329\tvalid_1's l2: 0.32437\n",
      "[1300]\ttraining's l2: 0.319959\tvalid_1's l2: 0.322981\n",
      "[1400]\ttraining's l2: 0.318657\tvalid_1's l2: 0.321665\n",
      "[1500]\ttraining's l2: 0.317351\tvalid_1's l2: 0.320349\n",
      "[1600]\ttraining's l2: 0.316116\tvalid_1's l2: 0.319107\n",
      "[1700]\ttraining's l2: 0.314894\tvalid_1's l2: 0.317869\n",
      "[1800]\ttraining's l2: 0.313695\tvalid_1's l2: 0.31667\n",
      "[1900]\ttraining's l2: 0.312552\tvalid_1's l2: 0.315483\n",
      "[2000]\ttraining's l2: 0.311402\tvalid_1's l2: 0.314361\n",
      "[2100]\ttraining's l2: 0.310301\tvalid_1's l2: 0.313257\n",
      "[2200]\ttraining's l2: 0.309197\tvalid_1's l2: 0.31215\n",
      "[2300]\ttraining's l2: 0.308115\tvalid_1's l2: 0.311072\n",
      "[2400]\ttraining's l2: 0.307056\tvalid_1's l2: 0.310001\n",
      "[2500]\ttraining's l2: 0.305997\tvalid_1's l2: 0.308914\n",
      "[2600]\ttraining's l2: 0.30495\tvalid_1's l2: 0.307863\n",
      "[2700]\ttraining's l2: 0.303922\tvalid_1's l2: 0.306836\n",
      "[2800]\ttraining's l2: 0.302933\tvalid_1's l2: 0.305838\n",
      "[2900]\ttraining's l2: 0.301958\tvalid_1's l2: 0.304872\n",
      "[3000]\ttraining's l2: 0.300996\tvalid_1's l2: 0.3039\n",
      "[3100]\ttraining's l2: 0.300043\tvalid_1's l2: 0.302935\n",
      "[3200]\ttraining's l2: 0.299093\tvalid_1's l2: 0.301991\n",
      "[3300]\ttraining's l2: 0.298167\tvalid_1's l2: 0.301072\n",
      "[3400]\ttraining's l2: 0.297252\tvalid_1's l2: 0.300142\n",
      "[3500]\ttraining's l2: 0.296366\tvalid_1's l2: 0.299237\n",
      "[3600]\ttraining's l2: 0.295485\tvalid_1's l2: 0.298354\n",
      "[3700]\ttraining's l2: 0.294619\tvalid_1's l2: 0.297485\n",
      "[3800]\ttraining's l2: 0.293741\tvalid_1's l2: 0.296626\n",
      "[3900]\ttraining's l2: 0.292854\tvalid_1's l2: 0.295753\n",
      "[4000]\ttraining's l2: 0.291983\tvalid_1's l2: 0.29489\n",
      "[4100]\ttraining's l2: 0.2911\tvalid_1's l2: 0.294007\n",
      "[4200]\ttraining's l2: 0.29026\tvalid_1's l2: 0.293179\n",
      "[4300]\ttraining's l2: 0.289439\tvalid_1's l2: 0.29235\n",
      "[4400]\ttraining's l2: 0.288586\tvalid_1's l2: 0.291503\n",
      "[4500]\ttraining's l2: 0.287762\tvalid_1's l2: 0.290683\n",
      "[4600]\ttraining's l2: 0.286951\tvalid_1's l2: 0.289861\n",
      "[4700]\ttraining's l2: 0.286142\tvalid_1's l2: 0.289025\n",
      "[4800]\ttraining's l2: 0.28533\tvalid_1's l2: 0.288225\n",
      "[4900]\ttraining's l2: 0.284542\tvalid_1's l2: 0.28745\n",
      "[5000]\ttraining's l2: 0.283738\tvalid_1's l2: 0.286661\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l2: 0.283738\tvalid_1's l2: 0.286661\n",
      "mean_14: 8843681.58\n",
      "mean_7: 4402735.17\n",
      "mean_30: 904215.79\n",
      "mean_4_dow3_2017: 824212.66\n",
      "mean_20_dow3_2017: 570838.40\n",
      "promo_3: 303657.81\n",
      "mean_21: 289875.98\n",
      "mean_60: 266545.19\n",
      "mean_3: 133149.90\n",
      "mean_4_dow4_2017: 107199.37\n",
      "std_14: 97260.51\n",
      "median_60: 90081.62\n",
      "std_16: 87529.31\n",
      "std_140: 80663.20\n",
      "std_7: 79996.33\n",
      "median_7: 78949.76\n",
      "mean_4_dow2_2017: 60365.45\n",
      "std_60: 56265.12\n",
      "promo_16: 50606.17\n",
      "std_3: 50526.39\n",
      "day_1_2017: 50451.58\n",
      "std_30: 49722.84\n",
      "mean_20_dow5_2017: 49641.39\n",
      "mean_4_dow6_2017: 48712.94\n",
      "mean_4_dow1_2017: 48414.50\n",
      "mean_140: 48185.80\n",
      "mean_20_dow2_2017: 47407.76\n",
      "mean_4_dow0_2017: 46809.49\n",
      "mean_4_dow5_2017: 46675.19\n",
      "promo_140: 46482.18\n",
      "mean_20_dow0_2017: 46313.10\n",
      "mean_20_dow4_2017: 45725.82\n",
      "mean_20_dow6_2017: 45638.26\n",
      "mean_16: 43493.73\n",
      "std_21: 43450.72\n",
      "mean_20_dow1_2017: 40883.02\n",
      "promo_60: 31006.44\n",
      "median_3: 30500.62\n",
      "promo_21: 30352.65\n",
      "promo_30: 29255.19\n",
      "promo_4: 28968.42\n",
      "promo_5: 27522.41\n",
      "median_140: 26360.83\n",
      "median_14: 25571.03\n",
      "median_30: 24688.59\n",
      "median_16: 22303.49\n",
      "promo_0: 18755.81\n",
      "median_21: 17263.10\n",
      "min_3: 16713.66\n",
      "promo_2: 16665.92\n",
      "promo_7: 15240.28\n",
      "promo_1: 13120.30\n",
      "max_3: 10686.13\n",
      "promo_6: 9422.03\n",
      "promo_14: 6165.07\n",
      "max_7: 5679.40\n",
      "max_14: 5354.66\n",
      "promo_9: 5000.57\n",
      "promo_8: 3986.62\n",
      "promo_10: 3703.28\n",
      "promo_15: 2949.46\n",
      "min_7: 2586.12\n",
      "promo_13: 2434.11\n",
      "max_16: 2034.71\n",
      "promo_12: 1893.71\n",
      "promo_11: 1712.08\n",
      "max_30: 1240.80\n",
      "max_21: 1085.33\n",
      "max_140: 950.66\n",
      "max_60: 757.02\n",
      "min_14: 98.15\n",
      "min_21: 52.75\n",
      "min_16: 26.02\n",
      "min_30: 2.84\n",
      "min_60: 0.00\n",
      "min_140: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Step 5\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's l2: 0.379458\tvalid_1's l2: 0.377392\n",
      "[200]\ttraining's l2: 0.352736\tvalid_1's l2: 0.350952\n",
      "[300]\ttraining's l2: 0.347015\tvalid_1's l2: 0.345005\n",
      "[400]\ttraining's l2: 0.343555\tvalid_1's l2: 0.341474\n",
      "[500]\ttraining's l2: 0.341082\tvalid_1's l2: 0.339012\n",
      "[600]\ttraining's l2: 0.338958\tvalid_1's l2: 0.336885\n",
      "[700]\ttraining's l2: 0.337045\tvalid_1's l2: 0.334965\n",
      "[800]\ttraining's l2: 0.335231\tvalid_1's l2: 0.333194\n",
      "[900]\ttraining's l2: 0.333565\tvalid_1's l2: 0.331579\n",
      "[1000]\ttraining's l2: 0.331977\tvalid_1's l2: 0.329996\n",
      "[1100]\ttraining's l2: 0.330402\tvalid_1's l2: 0.328424\n",
      "[1200]\ttraining's l2: 0.328907\tvalid_1's l2: 0.326941\n",
      "[1300]\ttraining's l2: 0.327509\tvalid_1's l2: 0.325562\n",
      "[1400]\ttraining's l2: 0.326136\tvalid_1's l2: 0.324207\n",
      "[1500]\ttraining's l2: 0.324783\tvalid_1's l2: 0.322894\n",
      "[1600]\ttraining's l2: 0.323477\tvalid_1's l2: 0.321635\n",
      "[1700]\ttraining's l2: 0.322217\tvalid_1's l2: 0.320386\n",
      "[1800]\ttraining's l2: 0.320952\tvalid_1's l2: 0.319154\n",
      "[1900]\ttraining's l2: 0.31974\tvalid_1's l2: 0.317951\n",
      "[2000]\ttraining's l2: 0.318544\tvalid_1's l2: 0.31676\n",
      "[2100]\ttraining's l2: 0.317389\tvalid_1's l2: 0.315649\n",
      "[2200]\ttraining's l2: 0.316288\tvalid_1's l2: 0.314573\n",
      "[2300]\ttraining's l2: 0.315159\tvalid_1's l2: 0.313454\n",
      "[2400]\ttraining's l2: 0.314042\tvalid_1's l2: 0.312376\n",
      "[2500]\ttraining's l2: 0.312945\tvalid_1's l2: 0.31126\n",
      "[2600]\ttraining's l2: 0.311896\tvalid_1's l2: 0.310258\n",
      "[2700]\ttraining's l2: 0.310823\tvalid_1's l2: 0.309183\n",
      "[2800]\ttraining's l2: 0.309816\tvalid_1's l2: 0.308185\n",
      "[2900]\ttraining's l2: 0.308804\tvalid_1's l2: 0.307181\n",
      "[3000]\ttraining's l2: 0.307791\tvalid_1's l2: 0.306181\n",
      "[3100]\ttraining's l2: 0.306786\tvalid_1's l2: 0.305205\n",
      "[3200]\ttraining's l2: 0.305816\tvalid_1's l2: 0.304251\n",
      "[3300]\ttraining's l2: 0.304833\tvalid_1's l2: 0.303319\n",
      "[3400]\ttraining's l2: 0.303884\tvalid_1's l2: 0.302386\n",
      "[3500]\ttraining's l2: 0.302941\tvalid_1's l2: 0.301441\n",
      "[3600]\ttraining's l2: 0.302007\tvalid_1's l2: 0.300513\n",
      "[3700]\ttraining's l2: 0.301079\tvalid_1's l2: 0.299601\n",
      "[3800]\ttraining's l2: 0.300139\tvalid_1's l2: 0.298684\n",
      "[3900]\ttraining's l2: 0.299242\tvalid_1's l2: 0.297814\n",
      "[4000]\ttraining's l2: 0.298351\tvalid_1's l2: 0.296936\n",
      "[4100]\ttraining's l2: 0.297455\tvalid_1's l2: 0.296046\n",
      "[4200]\ttraining's l2: 0.296563\tvalid_1's l2: 0.295158\n",
      "[4300]\ttraining's l2: 0.29571\tvalid_1's l2: 0.294325\n",
      "[4400]\ttraining's l2: 0.294871\tvalid_1's l2: 0.29349\n",
      "[4500]\ttraining's l2: 0.294022\tvalid_1's l2: 0.292656\n",
      "[4600]\ttraining's l2: 0.293199\tvalid_1's l2: 0.291866\n",
      "[4700]\ttraining's l2: 0.292358\tvalid_1's l2: 0.291029\n",
      "[4800]\ttraining's l2: 0.291531\tvalid_1's l2: 0.290213\n",
      "[4900]\ttraining's l2: 0.290723\tvalid_1's l2: 0.289424\n",
      "[5000]\ttraining's l2: 0.289902\tvalid_1's l2: 0.288635\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l2: 0.289902\tvalid_1's l2: 0.288635\n",
      "mean_14: 7677816.35\n",
      "mean_4_dow4_2017: 5699892.85\n",
      "mean_7: 1814218.37\n",
      "mean_20_dow4_2017: 825571.31\n",
      "mean_3: 633475.08\n",
      "promo_4: 329704.57\n",
      "mean_16: 329109.95\n",
      "mean_30: 282159.87\n",
      "mean_21: 270299.70\n",
      "mean_60: 141373.64\n",
      "mean_4_dow3_2017: 101959.57\n",
      "std_140: 84291.59\n",
      "std_16: 72033.37\n",
      "median_60: 67141.71\n",
      "std_7: 66081.16\n",
      "std_21: 64040.49\n",
      "std_60: 59239.58\n",
      "std_30: 57561.55\n",
      "std_3: 55005.63\n",
      "median_7: 53311.73\n",
      "std_14: 53177.82\n",
      "mean_20_dow3_2017: 52583.43\n",
      "median_3: 52142.23\n",
      "mean_20_dow1_2017: 52069.87\n",
      "mean_4_dow0_2017: 50644.52\n",
      "mean_4_dow1_2017: 50351.33\n",
      "mean_4_dow5_2017: 49899.15\n",
      "mean_20_dow2_2017: 48909.52\n",
      "mean_20_dow0_2017: 48673.09\n",
      "mean_4_dow6_2017: 48325.53\n",
      "mean_4_dow2_2017: 47108.01\n",
      "mean_20_dow5_2017: 46575.00\n",
      "promo_140: 45866.27\n",
      "mean_140: 44315.68\n",
      "mean_20_dow6_2017: 44067.47\n",
      "promo_16: 42228.92\n",
      "day_1_2017: 41132.45\n",
      "median_140: 35448.04\n",
      "promo_5: 32908.40\n",
      "promo_60: 30948.98\n",
      "promo_30: 30175.28\n",
      "promo_3: 28917.08\n",
      "promo_21: 28448.23\n",
      "median_30: 26181.90\n",
      "median_14: 25414.36\n",
      "median_16: 24898.67\n",
      "promo_7: 22740.17\n",
      "max_3: 22347.38\n",
      "median_21: 18258.14\n",
      "min_3: 17641.78\n",
      "promo_2: 15094.73\n",
      "promo_6: 14862.93\n",
      "promo_0: 14839.09\n",
      "promo_11: 13440.31\n",
      "promo_1: 11967.14\n",
      "promo_14: 8710.08\n",
      "promo_9: 5988.68\n",
      "max_14: 5359.25\n",
      "promo_8: 5189.26\n",
      "promo_13: 4643.44\n",
      "max_7: 3556.68\n",
      "promo_15: 3485.87\n",
      "promo_10: 3167.78\n",
      "promo_12: 2310.82\n",
      "min_7: 1794.45\n",
      "max_16: 1560.33\n",
      "max_21: 1092.31\n",
      "max_140: 1016.41\n",
      "max_30: 738.18\n",
      "max_60: 732.61\n",
      "min_14: 204.57\n",
      "min_21: 55.64\n",
      "min_16: 46.27\n",
      "min_30: 2.70\n",
      "min_60: 0.00\n",
      "min_140: 0.00\n",
      "==================================================\n",
      "Step 6\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's l2: 0.373653\tvalid_1's l2: 0.379907\n",
      "[200]\ttraining's l2: 0.35208\tvalid_1's l2: 0.353493\n",
      "[300]\ttraining's l2: 0.347434\tvalid_1's l2: 0.34803\n",
      "[400]\ttraining's l2: 0.344378\tvalid_1's l2: 0.344851\n",
      "[500]\ttraining's l2: 0.342082\tvalid_1's l2: 0.342549\n",
      "[600]\ttraining's l2: 0.340099\tvalid_1's l2: 0.340546\n",
      "[700]\ttraining's l2: 0.338243\tvalid_1's l2: 0.338731\n",
      "[800]\ttraining's l2: 0.33652\tvalid_1's l2: 0.337021\n",
      "[900]\ttraining's l2: 0.334917\tvalid_1's l2: 0.33546\n",
      "[1000]\ttraining's l2: 0.333365\tvalid_1's l2: 0.333907\n",
      "[1100]\ttraining's l2: 0.331858\tvalid_1's l2: 0.332392\n",
      "[1200]\ttraining's l2: 0.330377\tvalid_1's l2: 0.330958\n",
      "[1300]\ttraining's l2: 0.328977\tvalid_1's l2: 0.329601\n",
      "[1400]\ttraining's l2: 0.327606\tvalid_1's l2: 0.328266\n",
      "[1500]\ttraining's l2: 0.326331\tvalid_1's l2: 0.326981\n",
      "[1600]\ttraining's l2: 0.32506\tvalid_1's l2: 0.325761\n",
      "[1700]\ttraining's l2: 0.323832\tvalid_1's l2: 0.324542\n",
      "[1800]\ttraining's l2: 0.3226\tvalid_1's l2: 0.323353\n",
      "[1900]\ttraining's l2: 0.321388\tvalid_1's l2: 0.322146\n",
      "[2000]\ttraining's l2: 0.320192\tvalid_1's l2: 0.320965\n",
      "[2100]\ttraining's l2: 0.319025\tvalid_1's l2: 0.319843\n",
      "[2200]\ttraining's l2: 0.317883\tvalid_1's l2: 0.318728\n",
      "[2300]\ttraining's l2: 0.316793\tvalid_1's l2: 0.317642\n",
      "[2400]\ttraining's l2: 0.315711\tvalid_1's l2: 0.31657\n",
      "[2500]\ttraining's l2: 0.314638\tvalid_1's l2: 0.315515\n",
      "[2600]\ttraining's l2: 0.313581\tvalid_1's l2: 0.314472\n",
      "[2700]\ttraining's l2: 0.31255\tvalid_1's l2: 0.313448\n",
      "[2800]\ttraining's l2: 0.311527\tvalid_1's l2: 0.312438\n",
      "[2900]\ttraining's l2: 0.310519\tvalid_1's l2: 0.311427\n",
      "[3000]\ttraining's l2: 0.309511\tvalid_1's l2: 0.310457\n",
      "[3100]\ttraining's l2: 0.308483\tvalid_1's l2: 0.309453\n",
      "[3200]\ttraining's l2: 0.30751\tvalid_1's l2: 0.308494\n",
      "[3300]\ttraining's l2: 0.306531\tvalid_1's l2: 0.30752\n",
      "[3400]\ttraining's l2: 0.305562\tvalid_1's l2: 0.306562\n",
      "[3500]\ttraining's l2: 0.304606\tvalid_1's l2: 0.305607\n",
      "[3600]\ttraining's l2: 0.303661\tvalid_1's l2: 0.304667\n",
      "[3700]\ttraining's l2: 0.302741\tvalid_1's l2: 0.303773\n",
      "[3800]\ttraining's l2: 0.301828\tvalid_1's l2: 0.302858\n",
      "[3900]\ttraining's l2: 0.300907\tvalid_1's l2: 0.301936\n",
      "[4000]\ttraining's l2: 0.300016\tvalid_1's l2: 0.301061\n",
      "[4100]\ttraining's l2: 0.299114\tvalid_1's l2: 0.30017\n",
      "[4200]\ttraining's l2: 0.298246\tvalid_1's l2: 0.299309\n",
      "[4300]\ttraining's l2: 0.2974\tvalid_1's l2: 0.298503\n",
      "[4400]\ttraining's l2: 0.296518\tvalid_1's l2: 0.29762\n",
      "[4500]\ttraining's l2: 0.295663\tvalid_1's l2: 0.296775\n",
      "[4600]\ttraining's l2: 0.294804\tvalid_1's l2: 0.29594\n",
      "[4700]\ttraining's l2: 0.293956\tvalid_1's l2: 0.295125\n",
      "[4800]\ttraining's l2: 0.293127\tvalid_1's l2: 0.294295\n",
      "[4900]\ttraining's l2: 0.292283\tvalid_1's l2: 0.293455\n",
      "[5000]\ttraining's l2: 0.291444\tvalid_1's l2: 0.292613\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l2: 0.291444\tvalid_1's l2: 0.292613\n",
      "mean_16: 5826641.82\n",
      "mean_14: 3145821.42\n",
      "mean_7: 2540806.41\n",
      "mean_30: 995666.49\n",
      "mean_3: 569335.46\n",
      "promo_5: 323727.26\n",
      "mean_60: 297100.12\n",
      "mean_4_dow5_2017: 280402.78\n",
      "mean_20_dow5_2017: 273383.89\n",
      "median_60: 267689.12\n",
      "mean_21: 163978.15\n",
      "std_140: 76138.09\n",
      "std_16: 64858.60\n",
      "mean_4_dow6_2017: 61879.18\n",
      "std_21: 59695.03\n",
      "std_60: 59204.88\n",
      "median_30: 57687.13\n",
      "std_7: 56819.23\n",
      "std_30: 55594.21\n",
      "std_3: 55294.22\n",
      "mean_4_dow4_2017: 54774.91\n",
      "mean_4_dow1_2017: 52801.83\n",
      "mean_20_dow6_2017: 52620.17\n",
      "mean_4_dow0_2017: 51724.16\n",
      "mean_4_dow3_2017: 51212.45\n",
      "mean_4_dow2_2017: 49404.61\n",
      "promo_16: 49303.30\n",
      "median_3: 48506.52\n",
      "mean_20_dow3_2017: 46369.84\n",
      "mean_140: 46020.88\n",
      "mean_20_dow2_2017: 45963.97\n",
      "mean_20_dow0_2017: 45116.72\n",
      "std_14: 43467.75\n",
      "mean_20_dow1_2017: 43168.35\n",
      "mean_20_dow4_2017: 42946.88\n",
      "promo_3: 42666.54\n",
      "day_1_2017: 42571.59\n",
      "median_7: 41261.88\n",
      "promo_140: 39083.81\n",
      "promo_6: 34254.56\n",
      "median_140: 33019.87\n",
      "promo_30: 31722.65\n",
      "promo_60: 30451.60\n",
      "promo_7: 28462.67\n",
      "median_14: 28287.02\n",
      "promo_21: 27947.30\n",
      "median_16: 24500.28\n",
      "median_21: 19411.94\n",
      "min_3: 18420.22\n",
      "promo_0: 13841.20\n",
      "promo_4: 12838.68\n",
      "max_3: 10909.01\n",
      "promo_1: 9481.24\n",
      "promo_2: 8011.64\n",
      "promo_8: 7931.87\n",
      "promo_14: 6901.89\n",
      "promo_9: 6055.00\n",
      "promo_12: 4559.82\n",
      "promo_10: 4484.35\n",
      "max_7: 4253.32\n",
      "promo_13: 4251.21\n",
      "promo_11: 3545.55\n",
      "max_14: 3060.38\n",
      "promo_15: 2565.78\n",
      "min_7: 2135.10\n",
      "max_16: 1392.27\n",
      "max_30: 1361.51\n",
      "max_140: 1053.79\n",
      "max_21: 793.66\n",
      "min_14: 573.61\n",
      "max_60: 543.53\n",
      "min_21: 46.06\n",
      "min_16: 33.18\n",
      "min_30: 3.40\n",
      "min_60: 3.18\n",
      "min_140: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Step 7\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's l2: 0.373432\tvalid_1's l2: 0.435027\n",
      "[200]\ttraining's l2: 0.352296\tvalid_1's l2: 0.400639\n",
      "[300]\ttraining's l2: 0.34744\tvalid_1's l2: 0.39351\n",
      "[400]\ttraining's l2: 0.344406\tvalid_1's l2: 0.389667\n",
      "[500]\ttraining's l2: 0.342142\tvalid_1's l2: 0.386971\n",
      "[600]\ttraining's l2: 0.340201\tvalid_1's l2: 0.384644\n",
      "[700]\ttraining's l2: 0.338342\tvalid_1's l2: 0.382525\n",
      "[800]\ttraining's l2: 0.336657\tvalid_1's l2: 0.380526\n",
      "[900]\ttraining's l2: 0.335052\tvalid_1's l2: 0.378664\n",
      "[1000]\ttraining's l2: 0.33353\tvalid_1's l2: 0.376877\n",
      "[1100]\ttraining's l2: 0.332049\tvalid_1's l2: 0.375196\n",
      "[1200]\ttraining's l2: 0.330606\tvalid_1's l2: 0.373514\n",
      "[1300]\ttraining's l2: 0.32924\tvalid_1's l2: 0.371937\n",
      "[1400]\ttraining's l2: 0.327872\tvalid_1's l2: 0.370371\n",
      "[1500]\ttraining's l2: 0.326581\tvalid_1's l2: 0.368946\n",
      "[1600]\ttraining's l2: 0.325312\tvalid_1's l2: 0.367503\n",
      "[1700]\ttraining's l2: 0.324066\tvalid_1's l2: 0.366063\n",
      "[1800]\ttraining's l2: 0.322822\tvalid_1's l2: 0.364612\n",
      "[1900]\ttraining's l2: 0.32164\tvalid_1's l2: 0.363228\n",
      "[2000]\ttraining's l2: 0.320479\tvalid_1's l2: 0.361901\n",
      "[2100]\ttraining's l2: 0.319324\tvalid_1's l2: 0.360573\n",
      "[2200]\ttraining's l2: 0.318194\tvalid_1's l2: 0.35924\n",
      "[2300]\ttraining's l2: 0.317083\tvalid_1's l2: 0.358037\n",
      "[2400]\ttraining's l2: 0.315989\tvalid_1's l2: 0.356781\n",
      "[2500]\ttraining's l2: 0.314918\tvalid_1's l2: 0.355574\n",
      "[2600]\ttraining's l2: 0.313843\tvalid_1's l2: 0.354319\n",
      "[2700]\ttraining's l2: 0.312795\tvalid_1's l2: 0.353127\n",
      "[2800]\ttraining's l2: 0.311752\tvalid_1's l2: 0.351871\n",
      "[2900]\ttraining's l2: 0.310725\tvalid_1's l2: 0.35074\n",
      "[3000]\ttraining's l2: 0.309738\tvalid_1's l2: 0.349625\n",
      "[3100]\ttraining's l2: 0.30874\tvalid_1's l2: 0.348466\n",
      "[3200]\ttraining's l2: 0.307768\tvalid_1's l2: 0.347389\n",
      "[3300]\ttraining's l2: 0.306802\tvalid_1's l2: 0.346298\n",
      "[3400]\ttraining's l2: 0.305861\tvalid_1's l2: 0.34522\n",
      "[3500]\ttraining's l2: 0.304897\tvalid_1's l2: 0.34415\n",
      "[3600]\ttraining's l2: 0.303969\tvalid_1's l2: 0.343098\n",
      "[3700]\ttraining's l2: 0.303046\tvalid_1's l2: 0.342044\n",
      "[3800]\ttraining's l2: 0.302127\tvalid_1's l2: 0.341002\n",
      "[3900]\ttraining's l2: 0.301215\tvalid_1's l2: 0.339951\n",
      "[4000]\ttraining's l2: 0.300327\tvalid_1's l2: 0.338969\n",
      "[4100]\ttraining's l2: 0.299423\tvalid_1's l2: 0.337958\n",
      "[4200]\ttraining's l2: 0.298522\tvalid_1's l2: 0.336944\n",
      "[4300]\ttraining's l2: 0.297652\tvalid_1's l2: 0.335976\n",
      "[4400]\ttraining's l2: 0.296792\tvalid_1's l2: 0.335017\n",
      "[4500]\ttraining's l2: 0.29592\tvalid_1's l2: 0.334075\n",
      "[4600]\ttraining's l2: 0.295079\tvalid_1's l2: 0.33313\n",
      "[4700]\ttraining's l2: 0.294214\tvalid_1's l2: 0.332168\n",
      "[4800]\ttraining's l2: 0.293367\tvalid_1's l2: 0.331234\n",
      "[4900]\ttraining's l2: 0.292533\tvalid_1's l2: 0.330275\n",
      "[5000]\ttraining's l2: 0.291673\tvalid_1's l2: 0.32933\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l2: 0.291673\tvalid_1's l2: 0.32933\n",
      "mean_16: 4955430.15\n",
      "mean_30: 2463575.28\n",
      "mean_7: 2292232.64\n",
      "mean_14: 1938486.86\n",
      "mean_4_dow6_2017: 641356.51\n",
      "promo_6: 489562.57\n",
      "mean_20_dow6_2017: 397775.34\n",
      "mean_3: 303070.99\n",
      "mean_60: 161696.55\n",
      "mean_21: 156396.56\n",
      "median_60: 101351.56\n",
      "std_140: 85257.07\n",
      "mean_4_dow5_2017: 83820.65\n",
      "mean_20_dow5_2017: 69118.94\n",
      "std_30: 66538.30\n",
      "median_3: 61291.85\n",
      "std_60: 59667.66\n",
      "day_1_2017: 58273.10\n",
      "std_7: 57480.06\n",
      "mean_4_dow1_2017: 54255.47\n",
      "promo_16: 53632.98\n",
      "mean_140: 53171.04\n",
      "std_3: 52563.13\n",
      "std_21: 51763.51\n",
      "mean_4_dow4_2017: 51485.26\n",
      "mean_4_dow0_2017: 50943.98\n",
      "mean_4_dow3_2017: 50890.04\n",
      "mean_20_dow1_2017: 50200.46\n",
      "mean_4_dow2_2017: 50063.45\n",
      "std_16: 48162.91\n",
      "median_7: 47471.56\n",
      "mean_20_dow0_2017: 46156.74\n",
      "std_14: 45760.64\n",
      "median_30: 45387.63\n",
      "mean_20_dow2_2017: 44332.95\n",
      "promo_140: 43799.78\n",
      "mean_20_dow3_2017: 43783.39\n",
      "mean_20_dow4_2017: 43396.07\n",
      "promo_7: 36870.94\n",
      "promo_30: 35628.47\n",
      "median_140: 33096.98\n",
      "promo_60: 32526.78\n",
      "promo_3: 32398.51\n",
      "median_14: 27587.54\n",
      "promo_21: 27296.26\n",
      "max_3: 26681.18\n",
      "promo_13: 25333.16\n",
      "promo_5: 25069.28\n",
      "median_16: 24145.71\n",
      "median_21: 19318.34\n",
      "promo_0: 16198.65\n",
      "min_3: 12888.17\n",
      "promo_1: 10385.86\n",
      "promo_14: 9530.23\n",
      "promo_9: 8914.98\n",
      "promo_4: 8637.20\n",
      "promo_8: 6668.92\n",
      "promo_2: 6030.99\n",
      "max_7: 4477.46\n",
      "promo_15: 4450.97\n",
      "promo_11: 4007.52\n",
      "max_14: 3010.65\n",
      "promo_12: 2606.36\n",
      "min_7: 2397.18\n",
      "promo_10: 2161.56\n",
      "max_16: 1389.56\n",
      "max_140: 1035.49\n",
      "max_30: 952.63\n",
      "max_21: 807.70\n",
      "max_60: 736.83\n",
      "min_14: 624.55\n",
      "min_16: 41.89\n",
      "min_21: 38.73\n",
      "min_30: 2.76\n",
      "min_60: 2.09\n",
      "min_140: 0.00\n",
      "==================================================\n",
      "Step 8\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's l2: 0.358256\tvalid_1's l2: 0.405838\n",
      "[200]\ttraining's l2: 0.335499\tvalid_1's l2: 0.372065\n",
      "[300]\ttraining's l2: 0.330686\tvalid_1's l2: 0.364915\n",
      "[400]\ttraining's l2: 0.327707\tvalid_1's l2: 0.361041\n",
      "[500]\ttraining's l2: 0.325626\tvalid_1's l2: 0.358753\n",
      "[600]\ttraining's l2: 0.323755\tvalid_1's l2: 0.356614\n",
      "[700]\ttraining's l2: 0.322093\tvalid_1's l2: 0.354805\n",
      "[800]\ttraining's l2: 0.320523\tvalid_1's l2: 0.352963\n",
      "[900]\ttraining's l2: 0.319057\tvalid_1's l2: 0.351318\n",
      "[1000]\ttraining's l2: 0.317586\tvalid_1's l2: 0.349711\n",
      "[1100]\ttraining's l2: 0.316205\tvalid_1's l2: 0.348127\n",
      "[1200]\ttraining's l2: 0.314862\tvalid_1's l2: 0.34663\n",
      "[1300]\ttraining's l2: 0.313584\tvalid_1's l2: 0.345212\n",
      "[1400]\ttraining's l2: 0.312299\tvalid_1's l2: 0.343809\n",
      "[1500]\ttraining's l2: 0.311099\tvalid_1's l2: 0.34249\n",
      "[1600]\ttraining's l2: 0.309885\tvalid_1's l2: 0.341119\n",
      "[1700]\ttraining's l2: 0.308704\tvalid_1's l2: 0.339819\n",
      "[1800]\ttraining's l2: 0.307601\tvalid_1's l2: 0.338592\n",
      "[1900]\ttraining's l2: 0.306495\tvalid_1's l2: 0.337343\n",
      "[2000]\ttraining's l2: 0.305381\tvalid_1's l2: 0.336102\n",
      "[2100]\ttraining's l2: 0.304278\tvalid_1's l2: 0.3349\n",
      "[2200]\ttraining's l2: 0.303228\tvalid_1's l2: 0.333713\n",
      "[2300]\ttraining's l2: 0.302199\tvalid_1's l2: 0.332572\n",
      "[2400]\ttraining's l2: 0.301192\tvalid_1's l2: 0.331504\n",
      "[2500]\ttraining's l2: 0.300187\tvalid_1's l2: 0.330416\n",
      "[2600]\ttraining's l2: 0.299179\tvalid_1's l2: 0.329286\n",
      "[2700]\ttraining's l2: 0.29818\tvalid_1's l2: 0.328212\n",
      "[2800]\ttraining's l2: 0.297188\tvalid_1's l2: 0.327136\n",
      "[2900]\ttraining's l2: 0.296221\tvalid_1's l2: 0.326049\n",
      "[3000]\ttraining's l2: 0.29529\tvalid_1's l2: 0.32502\n",
      "[3100]\ttraining's l2: 0.294345\tvalid_1's l2: 0.323959\n",
      "[3200]\ttraining's l2: 0.293413\tvalid_1's l2: 0.32295\n",
      "[3300]\ttraining's l2: 0.29249\tvalid_1's l2: 0.321922\n",
      "[3400]\ttraining's l2: 0.291581\tvalid_1's l2: 0.32095\n",
      "[3500]\ttraining's l2: 0.290679\tvalid_1's l2: 0.319973\n",
      "[3600]\ttraining's l2: 0.28978\tvalid_1's l2: 0.319012\n",
      "[3700]\ttraining's l2: 0.288887\tvalid_1's l2: 0.318043\n",
      "[3800]\ttraining's l2: 0.288042\tvalid_1's l2: 0.317084\n",
      "[3900]\ttraining's l2: 0.287182\tvalid_1's l2: 0.31616\n",
      "[4000]\ttraining's l2: 0.286337\tvalid_1's l2: 0.315239\n",
      "[4100]\ttraining's l2: 0.285498\tvalid_1's l2: 0.314315\n",
      "[4200]\ttraining's l2: 0.284656\tvalid_1's l2: 0.313398\n",
      "[4300]\ttraining's l2: 0.283812\tvalid_1's l2: 0.312485\n",
      "[4400]\ttraining's l2: 0.282981\tvalid_1's l2: 0.311573\n",
      "[4500]\ttraining's l2: 0.282157\tvalid_1's l2: 0.310703\n",
      "[4600]\ttraining's l2: 0.281342\tvalid_1's l2: 0.309821\n",
      "[4700]\ttraining's l2: 0.280527\tvalid_1's l2: 0.308945\n",
      "[4800]\ttraining's l2: 0.279722\tvalid_1's l2: 0.308084\n",
      "[4900]\ttraining's l2: 0.278941\tvalid_1's l2: 0.307211\n",
      "[5000]\ttraining's l2: 0.278145\tvalid_1's l2: 0.306318\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l2: 0.278145\tvalid_1's l2: 0.306318\n",
      "mean_14: 5107446.28\n",
      "mean_30: 3074940.43\n",
      "mean_7: 2595721.16\n",
      "mean_21: 1293757.18\n",
      "promo_7: 657895.54\n",
      "mean_20_dow0_2017: 387525.15\n",
      "median_60: 355409.91\n",
      "mean_4_dow0_2017: 334405.84\n",
      "mean_16: 243670.88\n",
      "mean_60: 153482.85\n",
      "median_7: 134365.77\n",
      "median_14: 120187.25\n",
      "std_140: 101880.98\n",
      "promo_0: 95771.77\n",
      "mean_3: 75373.44\n",
      "mean_4_dow5_2017: 75173.59\n",
      "std_30: 74078.64\n",
      "median_30: 69072.39\n",
      "std_60: 65902.39\n",
      "promo_14: 64859.53\n",
      "std_21: 62602.51\n",
      "mean_140: 57912.09\n",
      "mean_20_dow2_2017: 56981.79\n",
      "std_7: 53532.48\n",
      "mean_4_dow6_2017: 51908.36\n",
      "promo_140: 49566.93\n",
      "mean_20_dow4_2017: 49375.96\n",
      "mean_4_dow2_2017: 49257.89\n",
      "day_1_2017: 48890.16\n",
      "mean_4_dow1_2017: 48444.52\n",
      "mean_4_dow3_2017: 46493.77\n",
      "mean_4_dow4_2017: 46015.18\n",
      "std_3: 45803.31\n",
      "std_14: 45362.71\n",
      "mean_20_dow6_2017: 44190.84\n",
      "std_16: 44082.62\n",
      "mean_20_dow1_2017: 42913.78\n",
      "mean_20_dow3_2017: 41533.23\n",
      "mean_20_dow5_2017: 41152.89\n",
      "promo_60: 38946.41\n",
      "promo_30: 37476.14\n",
      "promo_21: 34851.58\n",
      "median_3: 32822.88\n",
      "promo_16: 32463.61\n",
      "median_140: 30763.82\n",
      "median_16: 23092.28\n",
      "promo_3: 21383.93\n",
      "promo_6: 20384.85\n",
      "promo_5: 17855.68\n",
      "median_21: 15686.63\n",
      "promo_8: 15388.85\n",
      "promo_9: 13109.71\n",
      "max_3: 12965.34\n",
      "promo_15: 9845.15\n",
      "promo_2: 8204.49\n",
      "max_7: 7817.82\n",
      "max_14: 6825.12\n",
      "promo_4: 5758.89\n",
      "promo_1: 5647.19\n",
      "promo_11: 4689.84\n",
      "promo_13: 4650.83\n",
      "promo_10: 3432.58\n",
      "max_21: 3354.59\n",
      "promo_12: 3279.96\n",
      "min_3: 3196.40\n",
      "max_30: 2350.53\n",
      "max_140: 1748.34\n",
      "max_16: 1734.70\n",
      "min_7: 1649.38\n",
      "max_60: 1594.67\n",
      "min_14: 687.62\n",
      "min_16: 32.03\n",
      "min_21: 26.88\n",
      "min_60: 11.59\n",
      "min_30: 1.54\n",
      "min_140: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Step 9\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's l2: 0.364204\tvalid_1's l2: 0.39118\n",
      "[200]\ttraining's l2: 0.345185\tvalid_1's l2: 0.366948\n",
      "[300]\ttraining's l2: 0.3405\tvalid_1's l2: 0.361547\n",
      "[400]\ttraining's l2: 0.337639\tvalid_1's l2: 0.358518\n",
      "[500]\ttraining's l2: 0.335416\tvalid_1's l2: 0.356245\n",
      "[600]\ttraining's l2: 0.333542\tvalid_1's l2: 0.354339\n",
      "[700]\ttraining's l2: 0.331822\tvalid_1's l2: 0.352612\n",
      "[800]\ttraining's l2: 0.330216\tvalid_1's l2: 0.350946\n",
      "[900]\ttraining's l2: 0.328638\tvalid_1's l2: 0.349373\n",
      "[1000]\ttraining's l2: 0.327146\tvalid_1's l2: 0.347824\n",
      "[1100]\ttraining's l2: 0.325704\tvalid_1's l2: 0.346382\n",
      "[1200]\ttraining's l2: 0.324293\tvalid_1's l2: 0.344914\n",
      "[1300]\ttraining's l2: 0.322927\tvalid_1's l2: 0.343458\n",
      "[1400]\ttraining's l2: 0.321588\tvalid_1's l2: 0.342061\n",
      "[1500]\ttraining's l2: 0.320301\tvalid_1's l2: 0.340732\n",
      "[1600]\ttraining's l2: 0.319054\tvalid_1's l2: 0.33944\n",
      "[1700]\ttraining's l2: 0.317829\tvalid_1's l2: 0.338197\n",
      "[1800]\ttraining's l2: 0.316615\tvalid_1's l2: 0.336921\n",
      "[1900]\ttraining's l2: 0.315435\tvalid_1's l2: 0.335668\n",
      "[2000]\ttraining's l2: 0.314311\tvalid_1's l2: 0.334491\n",
      "[2100]\ttraining's l2: 0.313209\tvalid_1's l2: 0.33334\n",
      "[2200]\ttraining's l2: 0.312094\tvalid_1's l2: 0.332166\n",
      "[2300]\ttraining's l2: 0.311028\tvalid_1's l2: 0.331048\n",
      "[2400]\ttraining's l2: 0.309963\tvalid_1's l2: 0.329926\n",
      "[2500]\ttraining's l2: 0.308895\tvalid_1's l2: 0.328816\n",
      "[2600]\ttraining's l2: 0.307849\tvalid_1's l2: 0.327704\n",
      "[2700]\ttraining's l2: 0.306798\tvalid_1's l2: 0.326603\n",
      "[2800]\ttraining's l2: 0.305778\tvalid_1's l2: 0.3255\n",
      "[2900]\ttraining's l2: 0.304767\tvalid_1's l2: 0.324453\n",
      "[3000]\ttraining's l2: 0.303784\tvalid_1's l2: 0.323437\n",
      "[3100]\ttraining's l2: 0.302806\tvalid_1's l2: 0.322417\n",
      "[3200]\ttraining's l2: 0.301823\tvalid_1's l2: 0.321397\n",
      "[3300]\ttraining's l2: 0.300868\tvalid_1's l2: 0.320386\n",
      "[3400]\ttraining's l2: 0.299914\tvalid_1's l2: 0.319386\n",
      "[3500]\ttraining's l2: 0.298981\tvalid_1's l2: 0.318395\n",
      "[3600]\ttraining's l2: 0.298055\tvalid_1's l2: 0.317423\n",
      "[3700]\ttraining's l2: 0.297132\tvalid_1's l2: 0.316461\n",
      "[3800]\ttraining's l2: 0.296237\tvalid_1's l2: 0.315527\n",
      "[3900]\ttraining's l2: 0.295348\tvalid_1's l2: 0.314584\n",
      "[4000]\ttraining's l2: 0.294445\tvalid_1's l2: 0.313619\n",
      "[4100]\ttraining's l2: 0.293585\tvalid_1's l2: 0.312699\n",
      "[4200]\ttraining's l2: 0.292738\tvalid_1's l2: 0.3118\n",
      "[4300]\ttraining's l2: 0.29188\tvalid_1's l2: 0.310904\n",
      "[4400]\ttraining's l2: 0.291031\tvalid_1's l2: 0.310021\n",
      "[4500]\ttraining's l2: 0.290192\tvalid_1's l2: 0.309144\n",
      "[4600]\ttraining's l2: 0.289362\tvalid_1's l2: 0.308284\n",
      "[4700]\ttraining's l2: 0.288519\tvalid_1's l2: 0.307418\n",
      "[4800]\ttraining's l2: 0.287668\tvalid_1's l2: 0.30652\n",
      "[4900]\ttraining's l2: 0.286859\tvalid_1's l2: 0.305665\n",
      "[5000]\ttraining's l2: 0.286038\tvalid_1's l2: 0.304778\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l2: 0.286038\tvalid_1's l2: 0.304778\n",
      "mean_30: 4757627.72\n",
      "mean_14: 2076313.64\n",
      "mean_21: 1733603.18\n",
      "mean_7: 1387785.29\n",
      "median_60: 754069.40\n",
      "promo_8: 482664.82\n",
      "mean_20_dow1_2017: 320236.12\n",
      "mean_60: 261398.42\n",
      "mean_16: 207578.37\n",
      "median_30: 124330.85\n",
      "mean_4_dow1_2017: 119855.00\n",
      "std_140: 93792.15\n",
      "median_7: 87569.50\n",
      "std_60: 72861.77\n",
      "mean_3: 72718.56\n",
      "mean_20_dow2_2017: 68353.35\n",
      "mean_4_dow6_2017: 66446.62\n",
      "std_30: 60604.92\n",
      "mean_140: 57326.53\n",
      "std_21: 55713.68\n",
      "std_7: 53456.42\n",
      "mean_4_dow2_2017: 52685.86\n",
      "day_1_2017: 52455.95\n",
      "mean_4_dow0_2017: 51967.14\n",
      "mean_20_dow4_2017: 51897.25\n",
      "mean_4_dow4_2017: 50754.55\n",
      "promo_10: 50377.50\n",
      "mean_4_dow3_2017: 50141.89\n",
      "promo_140: 49177.44\n",
      "mean_4_dow5_2017: 49133.81\n",
      "std_3: 49006.18\n",
      "std_14: 47236.15\n",
      "mean_20_dow0_2017: 46754.10\n",
      "mean_20_dow6_2017: 45897.18\n",
      "mean_20_dow5_2017: 42713.55\n",
      "mean_20_dow3_2017: 42652.73\n",
      "promo_30: 40736.61\n",
      "std_16: 40532.06\n",
      "median_140: 36357.91\n",
      "promo_16: 34565.83\n",
      "promo_60: 32922.34\n",
      "median_3: 32187.35\n",
      "median_21: 31520.60\n",
      "median_14: 30384.96\n",
      "promo_21: 29661.44\n",
      "promo_7: 29646.42\n",
      "median_16: 24210.21\n",
      "promo_12: 19935.53\n",
      "promo_9: 15374.23\n",
      "promo_0: 12764.27\n",
      "promo_11: 11709.45\n",
      "promo_3: 11566.79\n",
      "promo_14: 9605.16\n",
      "promo_13: 8607.95\n",
      "promo_1: 7071.86\n",
      "promo_6: 6207.31\n",
      "min_3: 6099.97\n",
      "promo_4: 6062.09\n",
      "promo_2: 5438.38\n",
      "promo_15: 5383.98\n",
      "max_3: 4973.70\n",
      "max_7: 4911.09\n",
      "max_14: 4297.84\n",
      "promo_5: 3984.69\n",
      "max_30: 1443.63\n",
      "max_140: 1297.24\n",
      "max_16: 1123.24\n",
      "min_7: 934.58\n",
      "max_60: 719.65\n",
      "max_21: 708.70\n",
      "min_14: 339.85\n",
      "min_21: 44.69\n",
      "min_16: 26.23\n",
      "min_30: 10.73\n",
      "min_60: 0.00\n",
      "min_140: 0.00\n",
      "==================================================\n",
      "Step 10\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's l2: 0.370686\tvalid_1's l2: 0.386282\n",
      "[200]\ttraining's l2: 0.347483\tvalid_1's l2: 0.35996\n",
      "[300]\ttraining's l2: 0.342149\tvalid_1's l2: 0.354221\n",
      "[400]\ttraining's l2: 0.338933\tvalid_1's l2: 0.35102\n",
      "[500]\ttraining's l2: 0.336649\tvalid_1's l2: 0.348813\n",
      "[600]\ttraining's l2: 0.334707\tvalid_1's l2: 0.34695\n",
      "[700]\ttraining's l2: 0.332854\tvalid_1's l2: 0.345202\n",
      "[800]\ttraining's l2: 0.331132\tvalid_1's l2: 0.34358\n",
      "[900]\ttraining's l2: 0.329519\tvalid_1's l2: 0.342026\n",
      "[1000]\ttraining's l2: 0.328002\tvalid_1's l2: 0.340592\n",
      "[1100]\ttraining's l2: 0.326548\tvalid_1's l2: 0.339156\n",
      "[1200]\ttraining's l2: 0.325116\tvalid_1's l2: 0.337727\n",
      "[1300]\ttraining's l2: 0.323729\tvalid_1's l2: 0.336351\n",
      "[1400]\ttraining's l2: 0.322399\tvalid_1's l2: 0.33502\n",
      "[1500]\ttraining's l2: 0.321078\tvalid_1's l2: 0.333695\n",
      "[1600]\ttraining's l2: 0.319817\tvalid_1's l2: 0.332441\n",
      "[1700]\ttraining's l2: 0.318582\tvalid_1's l2: 0.331219\n",
      "[1800]\ttraining's l2: 0.317362\tvalid_1's l2: 0.330035\n",
      "[1900]\ttraining's l2: 0.316181\tvalid_1's l2: 0.328877\n",
      "[2000]\ttraining's l2: 0.315011\tvalid_1's l2: 0.327704\n",
      "[2100]\ttraining's l2: 0.313856\tvalid_1's l2: 0.326548\n",
      "[2200]\ttraining's l2: 0.312738\tvalid_1's l2: 0.325461\n",
      "[2300]\ttraining's l2: 0.311642\tvalid_1's l2: 0.324371\n",
      "[2400]\ttraining's l2: 0.310595\tvalid_1's l2: 0.323298\n",
      "[2500]\ttraining's l2: 0.309529\tvalid_1's l2: 0.322216\n",
      "[2600]\ttraining's l2: 0.308469\tvalid_1's l2: 0.321153\n",
      "[2700]\ttraining's l2: 0.307459\tvalid_1's l2: 0.320157\n",
      "[2800]\ttraining's l2: 0.306444\tvalid_1's l2: 0.319161\n",
      "[2900]\ttraining's l2: 0.305444\tvalid_1's l2: 0.318167\n",
      "[3000]\ttraining's l2: 0.304462\tvalid_1's l2: 0.317183\n",
      "[3100]\ttraining's l2: 0.303492\tvalid_1's l2: 0.316193\n",
      "[3200]\ttraining's l2: 0.302507\tvalid_1's l2: 0.315188\n",
      "[3300]\ttraining's l2: 0.30154\tvalid_1's l2: 0.314237\n",
      "[3400]\ttraining's l2: 0.300606\tvalid_1's l2: 0.3133\n",
      "[3500]\ttraining's l2: 0.299659\tvalid_1's l2: 0.312321\n",
      "[3600]\ttraining's l2: 0.298749\tvalid_1's l2: 0.311401\n",
      "[3700]\ttraining's l2: 0.297827\tvalid_1's l2: 0.310466\n",
      "[3800]\ttraining's l2: 0.296924\tvalid_1's l2: 0.309583\n",
      "[3900]\ttraining's l2: 0.296026\tvalid_1's l2: 0.308673\n",
      "[4000]\ttraining's l2: 0.295134\tvalid_1's l2: 0.307758\n",
      "[4100]\ttraining's l2: 0.294258\tvalid_1's l2: 0.306899\n",
      "[4200]\ttraining's l2: 0.29337\tvalid_1's l2: 0.305984\n",
      "[4300]\ttraining's l2: 0.292525\tvalid_1's l2: 0.305152\n",
      "[4400]\ttraining's l2: 0.291678\tvalid_1's l2: 0.304284\n",
      "[4500]\ttraining's l2: 0.290849\tvalid_1's l2: 0.303473\n",
      "[4600]\ttraining's l2: 0.290021\tvalid_1's l2: 0.302654\n",
      "[4700]\ttraining's l2: 0.289187\tvalid_1's l2: 0.301808\n",
      "[4800]\ttraining's l2: 0.288357\tvalid_1's l2: 0.300971\n",
      "[4900]\ttraining's l2: 0.28752\tvalid_1's l2: 0.300128\n",
      "[5000]\ttraining's l2: 0.286711\tvalid_1's l2: 0.299308\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l2: 0.286711\tvalid_1's l2: 0.299308\n",
      "mean_21: 4962386.75\n",
      "mean_30: 2222772.65\n",
      "mean_14: 1766552.58\n",
      "mean_7: 1615691.37\n",
      "mean_20_dow2_2017: 1337575.52\n",
      "mean_4_dow2_2017: 1075905.31\n",
      "promo_9: 570382.77\n",
      "median_60: 441568.41\n",
      "std_140: 136318.30\n",
      "mean_60: 91358.64\n",
      "std_30: 81078.56\n",
      "std_60: 78215.01\n",
      "mean_3: 75199.41\n",
      "promo_140: 69431.15\n",
      "mean_16: 68721.52\n",
      "mean_20_dow1_2017: 62778.38\n",
      "median_7: 62086.90\n",
      "std_21: 60832.29\n",
      "mean_20_dow4_2017: 58808.02\n",
      "std_7: 56809.44\n",
      "mean_4_dow3_2017: 56081.98\n",
      "mean_4_dow1_2017: 54722.06\n",
      "mean_4_dow0_2017: 52912.70\n",
      "mean_4_dow4_2017: 51252.82\n",
      "mean_4_dow6_2017: 48767.31\n",
      "std_14: 48719.23\n",
      "mean_4_dow5_2017: 48526.26\n",
      "promo_2: 47544.25\n",
      "mean_20_dow6_2017: 46316.63\n",
      "std_3: 46315.47\n",
      "promo_30: 46144.85\n",
      "mean_20_dow5_2017: 46056.97\n",
      "mean_20_dow0_2017: 45420.81\n",
      "mean_20_dow3_2017: 44182.58\n",
      "promo_10: 43279.07\n",
      "day_1_2017: 41756.20\n",
      "promo_16: 40747.95\n",
      "std_16: 40658.25\n",
      "promo_60: 38961.84\n",
      "mean_140: 36988.67\n",
      "promo_21: 35253.71\n",
      "median_140: 33435.27\n",
      "median_3: 33044.32\n",
      "promo_7: 30000.92\n",
      "promo_8: 28403.12\n",
      "median_30: 27560.12\n",
      "median_14: 26961.08\n",
      "median_16: 23257.42\n",
      "promo_14: 20808.71\n",
      "median_21: 18709.00\n",
      "max_3: 13453.16\n",
      "promo_12: 13040.58\n",
      "promo_11: 12548.80\n",
      "max_7: 9910.90\n",
      "promo_13: 9809.49\n",
      "promo_0: 8646.82\n",
      "promo_15: 7488.92\n",
      "max_14: 6569.72\n",
      "promo_1: 5239.44\n",
      "promo_6: 5231.73\n",
      "max_30: 3835.09\n",
      "promo_4: 3503.46\n",
      "min_3: 2858.36\n",
      "promo_3: 2539.49\n",
      "max_16: 2219.73\n",
      "max_140: 1908.21\n",
      "promo_5: 1799.41\n",
      "max_21: 1482.68\n",
      "max_60: 1021.44\n",
      "min_7: 1001.30\n",
      "min_14: 288.69\n",
      "min_21: 27.71\n",
      "min_30: 23.80\n",
      "min_16: 18.71\n",
      "min_60: 5.46\n",
      "min_140: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Step 11\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's l2: 0.396509\tvalid_1's l2: 0.392059\n",
      "[200]\ttraining's l2: 0.372077\tvalid_1's l2: 0.36813\n",
      "[300]\ttraining's l2: 0.366193\tvalid_1's l2: 0.362708\n",
      "[400]\ttraining's l2: 0.362467\tvalid_1's l2: 0.359561\n",
      "[500]\ttraining's l2: 0.359905\tvalid_1's l2: 0.357313\n",
      "[600]\ttraining's l2: 0.35771\tvalid_1's l2: 0.355351\n",
      "[700]\ttraining's l2: 0.355741\tvalid_1's l2: 0.353508\n",
      "[800]\ttraining's l2: 0.353907\tvalid_1's l2: 0.351864\n",
      "[900]\ttraining's l2: 0.352189\tvalid_1's l2: 0.3503\n",
      "[1000]\ttraining's l2: 0.350552\tvalid_1's l2: 0.34878\n",
      "[1100]\ttraining's l2: 0.34899\tvalid_1's l2: 0.347345\n",
      "[1200]\ttraining's l2: 0.347547\tvalid_1's l2: 0.346009\n",
      "[1300]\ttraining's l2: 0.346022\tvalid_1's l2: 0.344635\n",
      "[1400]\ttraining's l2: 0.344615\tvalid_1's l2: 0.343321\n",
      "[1500]\ttraining's l2: 0.343241\tvalid_1's l2: 0.34204\n",
      "[1600]\ttraining's l2: 0.341885\tvalid_1's l2: 0.340788\n",
      "[1700]\ttraining's l2: 0.340564\tvalid_1's l2: 0.33955\n",
      "[1800]\ttraining's l2: 0.339214\tvalid_1's l2: 0.338323\n",
      "[1900]\ttraining's l2: 0.337964\tvalid_1's l2: 0.337161\n",
      "[2000]\ttraining's l2: 0.336743\tvalid_1's l2: 0.335976\n",
      "[2100]\ttraining's l2: 0.335507\tvalid_1's l2: 0.334815\n",
      "[2200]\ttraining's l2: 0.334346\tvalid_1's l2: 0.333729\n",
      "[2300]\ttraining's l2: 0.333209\tvalid_1's l2: 0.332632\n",
      "[2400]\ttraining's l2: 0.332073\tvalid_1's l2: 0.33156\n",
      "[2500]\ttraining's l2: 0.330942\tvalid_1's l2: 0.330456\n",
      "[2600]\ttraining's l2: 0.329857\tvalid_1's l2: 0.329423\n",
      "[2700]\ttraining's l2: 0.328799\tvalid_1's l2: 0.328435\n",
      "[2800]\ttraining's l2: 0.327686\tvalid_1's l2: 0.327368\n",
      "[2900]\ttraining's l2: 0.326625\tvalid_1's l2: 0.326339\n",
      "[3000]\ttraining's l2: 0.32557\tvalid_1's l2: 0.325371\n",
      "[3100]\ttraining's l2: 0.324552\tvalid_1's l2: 0.324394\n",
      "[3200]\ttraining's l2: 0.323529\tvalid_1's l2: 0.323428\n",
      "[3300]\ttraining's l2: 0.322533\tvalid_1's l2: 0.322476\n",
      "[3400]\ttraining's l2: 0.321518\tvalid_1's l2: 0.321513\n",
      "[3500]\ttraining's l2: 0.320546\tvalid_1's l2: 0.320581\n",
      "[3600]\ttraining's l2: 0.319578\tvalid_1's l2: 0.319668\n",
      "[3700]\ttraining's l2: 0.318614\tvalid_1's l2: 0.318758\n",
      "[3800]\ttraining's l2: 0.317676\tvalid_1's l2: 0.317836\n",
      "[3900]\ttraining's l2: 0.31673\tvalid_1's l2: 0.316938\n",
      "[4000]\ttraining's l2: 0.315806\tvalid_1's l2: 0.316052\n",
      "[4100]\ttraining's l2: 0.314881\tvalid_1's l2: 0.315191\n",
      "[4200]\ttraining's l2: 0.313978\tvalid_1's l2: 0.314326\n",
      "[4300]\ttraining's l2: 0.313063\tvalid_1's l2: 0.313461\n",
      "[4400]\ttraining's l2: 0.31217\tvalid_1's l2: 0.312591\n",
      "[4500]\ttraining's l2: 0.311273\tvalid_1's l2: 0.311724\n",
      "[4600]\ttraining's l2: 0.3104\tvalid_1's l2: 0.310877\n",
      "[4700]\ttraining's l2: 0.309526\tvalid_1's l2: 0.310042\n",
      "[4800]\ttraining's l2: 0.308666\tvalid_1's l2: 0.309224\n",
      "[4900]\ttraining's l2: 0.307818\tvalid_1's l2: 0.308409\n",
      "[5000]\ttraining's l2: 0.306968\tvalid_1's l2: 0.3076\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l2: 0.306968\tvalid_1's l2: 0.3076\n",
      "mean_21: 7056850.26\n",
      "mean_30: 3665187.03\n",
      "mean_7: 1667266.04\n",
      "mean_4_dow3_2017: 1020209.04\n",
      "mean_20_dow3_2017: 718320.19\n",
      "mean_60: 637347.76\n",
      "mean_14: 480839.60\n",
      "promo_10: 439831.90\n",
      "median_60: 228408.73\n",
      "mean_16: 121407.45\n",
      "std_140: 113182.77\n",
      "mean_4_dow4_2017: 110498.40\n",
      "mean_3: 107931.88\n",
      "std_30: 74569.43\n",
      "std_21: 72545.70\n",
      "std_60: 63019.51\n",
      "mean_4_dow2_2017: 63009.65\n",
      "promo_140: 61705.70\n",
      "std_7: 58111.41\n",
      "mean_20_dow2_2017: 56210.68\n",
      "mean_140: 54197.18\n",
      "mean_20_dow5_2017: 53644.37\n",
      "mean_4_dow0_2017: 53472.03\n",
      "mean_20_dow0_2017: 53373.17\n",
      "mean_20_dow6_2017: 52152.31\n",
      "mean_4_dow6_2017: 51778.88\n",
      "mean_4_dow1_2017: 51754.89\n",
      "mean_20_dow4_2017: 51202.53\n",
      "median_7: 50780.33\n",
      "mean_4_dow5_2017: 50625.23\n",
      "mean_20_dow1_2017: 48938.53\n",
      "std_3: 48239.23\n",
      "std_14: 44706.97\n",
      "day_1_2017: 43291.22\n",
      "std_16: 42479.75\n",
      "promo_12: 38885.02\n",
      "promo_16: 37827.09\n",
      "median_3: 37244.31\n",
      "median_140: 37124.76\n",
      "promo_30: 36926.69\n",
      "promo_21: 36201.76\n",
      "promo_60: 34494.81\n",
      "promo_11: 31148.94\n",
      "promo_7: 31023.85\n",
      "promo_9: 28970.05\n",
      "promo_8: 27196.40\n",
      "median_14: 26051.91\n",
      "median_30: 25959.15\n",
      "promo_14: 25055.09\n",
      "median_16: 23067.48\n",
      "median_21: 18151.76\n",
      "promo_13: 14827.92\n",
      "promo_3: 9524.34\n",
      "promo_15: 7928.07\n",
      "max_14: 7287.80\n",
      "promo_0: 7031.15\n",
      "max_3: 6964.69\n",
      "promo_6: 6197.03\n",
      "max_7: 5279.06\n",
      "min_3: 5262.68\n",
      "promo_2: 5096.18\n",
      "promo_1: 4385.52\n",
      "promo_4: 3585.28\n",
      "promo_5: 2877.92\n",
      "max_30: 2454.09\n",
      "max_16: 1935.85\n",
      "max_140: 1820.41\n",
      "min_7: 1030.25\n",
      "max_60: 811.86\n",
      "max_21: 499.55\n",
      "min_14: 349.58\n",
      "min_30: 58.32\n",
      "min_21: 29.36\n",
      "min_16: 9.36\n",
      "min_60: 3.17\n",
      "min_140: 0.00\n",
      "==================================================\n",
      "Step 12\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's l2: 0.408623\tvalid_1's l2: 0.404345\n",
      "[200]\ttraining's l2: 0.381843\tvalid_1's l2: 0.379817\n",
      "[300]\ttraining's l2: 0.375644\tvalid_1's l2: 0.37426\n",
      "[400]\ttraining's l2: 0.371859\tvalid_1's l2: 0.370933\n",
      "[500]\ttraining's l2: 0.369216\tvalid_1's l2: 0.368612\n",
      "[600]\ttraining's l2: 0.366795\tvalid_1's l2: 0.36648\n",
      "[700]\ttraining's l2: 0.36469\tvalid_1's l2: 0.364561\n",
      "[800]\ttraining's l2: 0.362784\tvalid_1's l2: 0.36283\n",
      "[900]\ttraining's l2: 0.360958\tvalid_1's l2: 0.361177\n",
      "[1000]\ttraining's l2: 0.359272\tvalid_1's l2: 0.359666\n",
      "[1100]\ttraining's l2: 0.357587\tvalid_1's l2: 0.358077\n",
      "[1200]\ttraining's l2: 0.355991\tvalid_1's l2: 0.356604\n",
      "[1300]\ttraining's l2: 0.354485\tvalid_1's l2: 0.355225\n",
      "[1400]\ttraining's l2: 0.353023\tvalid_1's l2: 0.353862\n",
      "[1500]\ttraining's l2: 0.351566\tvalid_1's l2: 0.352474\n",
      "[1600]\ttraining's l2: 0.350173\tvalid_1's l2: 0.351212\n",
      "[1700]\ttraining's l2: 0.348761\tvalid_1's l2: 0.349915\n",
      "[1800]\ttraining's l2: 0.34745\tvalid_1's l2: 0.348675\n",
      "[1900]\ttraining's l2: 0.346136\tvalid_1's l2: 0.347422\n",
      "[2000]\ttraining's l2: 0.344852\tvalid_1's l2: 0.346166\n",
      "[2100]\ttraining's l2: 0.343557\tvalid_1's l2: 0.344958\n",
      "[2200]\ttraining's l2: 0.342339\tvalid_1's l2: 0.343827\n",
      "[2300]\ttraining's l2: 0.341136\tvalid_1's l2: 0.34266\n",
      "[2400]\ttraining's l2: 0.339918\tvalid_1's l2: 0.341543\n",
      "[2500]\ttraining's l2: 0.338783\tvalid_1's l2: 0.340467\n",
      "[2600]\ttraining's l2: 0.337651\tvalid_1's l2: 0.339336\n",
      "[2700]\ttraining's l2: 0.33654\tvalid_1's l2: 0.338248\n",
      "[2800]\ttraining's l2: 0.33544\tvalid_1's l2: 0.337201\n",
      "[2900]\ttraining's l2: 0.334336\tvalid_1's l2: 0.336155\n",
      "[3000]\ttraining's l2: 0.333224\tvalid_1's l2: 0.335132\n",
      "[3100]\ttraining's l2: 0.332144\tvalid_1's l2: 0.334095\n",
      "[3200]\ttraining's l2: 0.331054\tvalid_1's l2: 0.333045\n",
      "[3300]\ttraining's l2: 0.330027\tvalid_1's l2: 0.332061\n",
      "[3400]\ttraining's l2: 0.328958\tvalid_1's l2: 0.331042\n",
      "[3500]\ttraining's l2: 0.327943\tvalid_1's l2: 0.330052\n",
      "[3600]\ttraining's l2: 0.326942\tvalid_1's l2: 0.329092\n",
      "[3700]\ttraining's l2: 0.325944\tvalid_1's l2: 0.328141\n",
      "[3800]\ttraining's l2: 0.324986\tvalid_1's l2: 0.327235\n",
      "[3900]\ttraining's l2: 0.323989\tvalid_1's l2: 0.326261\n",
      "[4000]\ttraining's l2: 0.322998\tvalid_1's l2: 0.325305\n",
      "[4100]\ttraining's l2: 0.322042\tvalid_1's l2: 0.324388\n",
      "[4200]\ttraining's l2: 0.321097\tvalid_1's l2: 0.323471\n",
      "[4300]\ttraining's l2: 0.320164\tvalid_1's l2: 0.322588\n",
      "[4400]\ttraining's l2: 0.319237\tvalid_1's l2: 0.321689\n",
      "[4500]\ttraining's l2: 0.3183\tvalid_1's l2: 0.320784\n",
      "[4600]\ttraining's l2: 0.317402\tvalid_1's l2: 0.319928\n",
      "[4700]\ttraining's l2: 0.316497\tvalid_1's l2: 0.319081\n",
      "[4800]\ttraining's l2: 0.315596\tvalid_1's l2: 0.318216\n",
      "[4900]\ttraining's l2: 0.314733\tvalid_1's l2: 0.317372\n",
      "[5000]\ttraining's l2: 0.313839\tvalid_1's l2: 0.316494\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l2: 0.313839\tvalid_1's l2: 0.316494\n",
      "mean_4_dow4_2017: 6617960.51\n",
      "mean_21: 4574956.22\n",
      "mean_30: 2375390.83\n",
      "mean_20_dow4_2017: 1062430.53\n",
      "mean_7: 886256.48\n",
      "promo_11: 442026.27\n",
      "mean_60: 422284.23\n",
      "mean_3: 379693.53\n",
      "mean_14: 305007.55\n",
      "mean_16: 234431.30\n",
      "std_140: 127461.66\n",
      "median_60: 108264.12\n",
      "mean_4_dow3_2017: 88922.93\n",
      "std_60: 71776.84\n",
      "promo_140: 63792.34\n",
      "std_30: 62418.82\n",
      "std_21: 61990.14\n",
      "median_3: 60201.83\n",
      "promo_12: 59935.72\n",
      "mean_20_dow3_2017: 59646.70\n",
      "mean_140: 57421.13\n",
      "mean_20_dow1_2017: 57113.86\n",
      "mean_20_dow2_2017: 57038.52\n",
      "std_7: 55458.54\n",
      "mean_20_dow0_2017: 53493.11\n",
      "mean_4_dow0_2017: 52803.28\n",
      "mean_4_dow1_2017: 52775.14\n",
      "mean_4_dow5_2017: 52588.74\n",
      "std_3: 51698.35\n",
      "mean_20_dow5_2017: 50947.02\n",
      "mean_4_dow6_2017: 50827.31\n",
      "mean_4_dow2_2017: 50176.01\n",
      "std_14: 50048.35\n",
      "mean_20_dow6_2017: 49935.46\n",
      "median_140: 48406.65\n",
      "median_7: 48068.10\n",
      "std_16: 41298.83\n",
      "promo_21: 40924.43\n",
      "day_1_2017: 38529.44\n",
      "promo_30: 38237.84\n",
      "promo_60: 35712.27\n",
      "promo_14: 35070.01\n",
      "promo_16: 34003.15\n",
      "promo_10: 29488.03\n",
      "median_30: 28369.37\n",
      "median_14: 26803.60\n",
      "promo_9: 25892.81\n",
      "promo_13: 25364.58\n",
      "promo_7: 24613.88\n",
      "median_16: 24328.93\n",
      "median_21: 20684.93\n",
      "promo_4: 20546.84\n",
      "promo_8: 16060.98\n",
      "max_3: 12090.80\n",
      "promo_15: 9318.24\n",
      "promo_0: 9018.08\n",
      "max_14: 7434.06\n",
      "min_3: 7327.10\n",
      "promo_6: 5493.95\n",
      "max_7: 5208.41\n",
      "promo_2: 4442.12\n",
      "promo_1: 3963.60\n",
      "promo_5: 3553.74\n",
      "max_16: 2296.03\n",
      "promo_3: 2224.53\n",
      "max_30: 1577.89\n",
      "max_140: 1360.47\n",
      "min_7: 1148.47\n",
      "max_60: 1046.75\n",
      "max_21: 754.21\n",
      "min_14: 289.88\n",
      "min_16: 32.88\n",
      "min_21: 8.81\n",
      "min_60: 1.02\n",
      "min_30: 1.00\n",
      "min_140: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Step 13\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's l2: 0.396097\tvalid_1's l2: 0.389812\n",
      "[200]\ttraining's l2: 0.374168\tvalid_1's l2: 0.369592\n",
      "[300]\ttraining's l2: 0.368737\tvalid_1's l2: 0.364795\n",
      "[400]\ttraining's l2: 0.365466\tvalid_1's l2: 0.361984\n",
      "[500]\ttraining's l2: 0.362979\tvalid_1's l2: 0.359762\n",
      "[600]\ttraining's l2: 0.360851\tvalid_1's l2: 0.357879\n",
      "[700]\ttraining's l2: 0.358937\tvalid_1's l2: 0.356145\n",
      "[800]\ttraining's l2: 0.357178\tvalid_1's l2: 0.354591\n",
      "[900]\ttraining's l2: 0.355455\tvalid_1's l2: 0.353051\n",
      "[1000]\ttraining's l2: 0.353794\tvalid_1's l2: 0.351528\n",
      "[1100]\ttraining's l2: 0.352245\tvalid_1's l2: 0.350079\n",
      "[1200]\ttraining's l2: 0.350721\tvalid_1's l2: 0.348687\n",
      "[1300]\ttraining's l2: 0.349252\tvalid_1's l2: 0.347316\n",
      "[1400]\ttraining's l2: 0.34781\tvalid_1's l2: 0.345983\n",
      "[1500]\ttraining's l2: 0.346437\tvalid_1's l2: 0.34471\n",
      "[1600]\ttraining's l2: 0.345073\tvalid_1's l2: 0.343477\n",
      "[1700]\ttraining's l2: 0.343749\tvalid_1's l2: 0.342247\n",
      "[1800]\ttraining's l2: 0.342437\tvalid_1's l2: 0.341045\n",
      "[1900]\ttraining's l2: 0.341162\tvalid_1's l2: 0.339876\n",
      "[2000]\ttraining's l2: 0.339919\tvalid_1's l2: 0.338684\n",
      "[2100]\ttraining's l2: 0.33871\tvalid_1's l2: 0.337541\n",
      "[2200]\ttraining's l2: 0.337542\tvalid_1's l2: 0.336434\n",
      "[2300]\ttraining's l2: 0.336353\tvalid_1's l2: 0.335322\n",
      "[2400]\ttraining's l2: 0.335204\tvalid_1's l2: 0.334246\n",
      "[2500]\ttraining's l2: 0.334063\tvalid_1's l2: 0.333147\n",
      "[2600]\ttraining's l2: 0.332922\tvalid_1's l2: 0.332108\n",
      "[2700]\ttraining's l2: 0.331808\tvalid_1's l2: 0.331043\n",
      "[2800]\ttraining's l2: 0.330708\tvalid_1's l2: 0.329999\n",
      "[2900]\ttraining's l2: 0.329621\tvalid_1's l2: 0.328964\n",
      "[3000]\ttraining's l2: 0.328572\tvalid_1's l2: 0.327963\n",
      "[3100]\ttraining's l2: 0.327497\tvalid_1's l2: 0.326942\n",
      "[3200]\ttraining's l2: 0.326483\tvalid_1's l2: 0.325973\n",
      "[3300]\ttraining's l2: 0.325461\tvalid_1's l2: 0.325001\n",
      "[3400]\ttraining's l2: 0.324434\tvalid_1's l2: 0.324038\n",
      "[3500]\ttraining's l2: 0.323422\tvalid_1's l2: 0.323088\n",
      "[3600]\ttraining's l2: 0.322431\tvalid_1's l2: 0.322114\n",
      "[3700]\ttraining's l2: 0.321459\tvalid_1's l2: 0.321185\n",
      "[3800]\ttraining's l2: 0.320499\tvalid_1's l2: 0.320265\n",
      "[3900]\ttraining's l2: 0.319521\tvalid_1's l2: 0.319333\n",
      "[4000]\ttraining's l2: 0.318571\tvalid_1's l2: 0.318435\n",
      "[4100]\ttraining's l2: 0.317649\tvalid_1's l2: 0.317534\n",
      "[4200]\ttraining's l2: 0.316706\tvalid_1's l2: 0.316648\n",
      "[4300]\ttraining's l2: 0.315775\tvalid_1's l2: 0.315772\n",
      "[4400]\ttraining's l2: 0.31487\tvalid_1's l2: 0.314917\n",
      "[4500]\ttraining's l2: 0.313947\tvalid_1's l2: 0.314037\n",
      "[4600]\ttraining's l2: 0.313039\tvalid_1's l2: 0.313187\n",
      "[4700]\ttraining's l2: 0.312121\tvalid_1's l2: 0.312292\n",
      "[4800]\ttraining's l2: 0.311232\tvalid_1's l2: 0.311442\n",
      "[4900]\ttraining's l2: 0.31037\tvalid_1's l2: 0.310632\n",
      "[5000]\ttraining's l2: 0.309479\tvalid_1's l2: 0.309758\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l2: 0.309479\tvalid_1's l2: 0.309758\n",
      "mean_30: 5355589.07\n",
      "mean_21: 3149554.19\n",
      "mean_16: 2600665.27\n",
      "mean_60: 662666.07\n",
      "mean_7: 563011.62\n",
      "promo_12: 476216.13\n",
      "median_60: 452545.20\n",
      "mean_20_dow5_2017: 354925.53\n",
      "mean_3: 308395.99\n",
      "mean_14: 213987.14\n",
      "mean_4_dow5_2017: 213763.97\n",
      "std_140: 104136.08\n",
      "std_60: 71641.45\n",
      "std_30: 62633.08\n",
      "mean_20_dow6_2017: 60806.94\n",
      "mean_140: 57754.58\n",
      "mean_4_dow6_2017: 55046.12\n",
      "mean_4_dow0_2017: 54895.95\n",
      "mean_4_dow4_2017: 54727.04\n",
      "mean_4_dow1_2017: 54382.09\n",
      "mean_4_dow3_2017: 54380.90\n",
      "median_3: 53957.35\n",
      "mean_20_dow2_2017: 53405.87\n",
      "std_7: 52713.64\n",
      "std_21: 52226.24\n",
      "mean_4_dow2_2017: 52118.13\n",
      "mean_20_dow0_2017: 51598.92\n",
      "promo_140: 50808.30\n",
      "std_3: 50636.90\n",
      "mean_20_dow3_2017: 50167.36\n",
      "median_140: 49653.44\n",
      "mean_20_dow4_2017: 49352.90\n",
      "mean_20_dow1_2017: 48203.47\n",
      "day_1_2017: 46131.21\n",
      "std_16: 44070.67\n",
      "std_14: 43643.52\n",
      "median_16: 42716.40\n",
      "promo_21: 39328.20\n",
      "median_30: 37328.86\n",
      "promo_16: 37186.35\n",
      "promo_10: 37022.24\n",
      "promo_30: 35999.32\n",
      "promo_60: 34764.17\n",
      "median_7: 34699.63\n",
      "promo_13: 34212.46\n",
      "promo_14: 33734.78\n",
      "median_14: 28264.51\n",
      "promo_11: 21748.91\n",
      "median_21: 20065.45\n",
      "promo_7: 17223.23\n",
      "promo_15: 11955.76\n",
      "promo_9: 11889.58\n",
      "promo_8: 11285.34\n",
      "promo_0: 9072.00\n",
      "promo_5: 7159.95\n",
      "max_3: 6611.11\n",
      "promo_6: 6404.73\n",
      "min_3: 5632.57\n",
      "max_7: 5088.57\n",
      "promo_2: 4402.02\n",
      "promo_4: 3966.46\n",
      "promo_1: 3530.30\n",
      "max_14: 3350.40\n",
      "promo_3: 2662.00\n",
      "max_30: 1685.94\n",
      "min_7: 1451.20\n",
      "max_140: 1275.16\n",
      "max_16: 1271.83\n",
      "max_60: 991.50\n",
      "max_21: 361.29\n",
      "min_14: 186.56\n",
      "min_16: 26.09\n",
      "min_21: 23.81\n",
      "min_30: 15.18\n",
      "min_60: 9.52\n",
      "min_140: 0.00\n",
      "==================================================\n",
      "Step 14\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's l2: 0.393271\tvalid_1's l2: 0.375228\n",
      "[200]\ttraining's l2: 0.371675\tvalid_1's l2: 0.356474\n",
      "[300]\ttraining's l2: 0.366373\tvalid_1's l2: 0.351971\n",
      "[400]\ttraining's l2: 0.363066\tvalid_1's l2: 0.349148\n",
      "[500]\ttraining's l2: 0.360739\tvalid_1's l2: 0.347178\n",
      "[600]\ttraining's l2: 0.358675\tvalid_1's l2: 0.345385\n",
      "[700]\ttraining's l2: 0.356787\tvalid_1's l2: 0.343721\n",
      "[800]\ttraining's l2: 0.355028\tvalid_1's l2: 0.34221\n",
      "[900]\ttraining's l2: 0.353318\tvalid_1's l2: 0.34079\n",
      "[1000]\ttraining's l2: 0.351727\tvalid_1's l2: 0.339381\n",
      "[1100]\ttraining's l2: 0.350187\tvalid_1's l2: 0.338004\n",
      "[1200]\ttraining's l2: 0.348674\tvalid_1's l2: 0.336708\n",
      "[1300]\ttraining's l2: 0.347187\tvalid_1's l2: 0.335379\n",
      "[1400]\ttraining's l2: 0.345728\tvalid_1's l2: 0.334093\n",
      "[1500]\ttraining's l2: 0.344398\tvalid_1's l2: 0.332853\n",
      "[1600]\ttraining's l2: 0.343025\tvalid_1's l2: 0.331617\n",
      "[1700]\ttraining's l2: 0.341699\tvalid_1's l2: 0.330443\n",
      "[1800]\ttraining's l2: 0.340425\tvalid_1's l2: 0.329297\n",
      "[1900]\ttraining's l2: 0.339146\tvalid_1's l2: 0.328153\n",
      "[2000]\ttraining's l2: 0.337926\tvalid_1's l2: 0.327058\n",
      "[2100]\ttraining's l2: 0.336698\tvalid_1's l2: 0.325917\n",
      "[2200]\ttraining's l2: 0.335497\tvalid_1's l2: 0.324857\n",
      "[2300]\ttraining's l2: 0.334335\tvalid_1's l2: 0.323804\n",
      "[2400]\ttraining's l2: 0.333221\tvalid_1's l2: 0.322788\n",
      "[2500]\ttraining's l2: 0.332102\tvalid_1's l2: 0.321793\n",
      "[2600]\ttraining's l2: 0.330971\tvalid_1's l2: 0.320751\n",
      "[2700]\ttraining's l2: 0.329849\tvalid_1's l2: 0.319767\n",
      "[2800]\ttraining's l2: 0.328745\tvalid_1's l2: 0.318784\n",
      "[2900]\ttraining's l2: 0.327669\tvalid_1's l2: 0.317833\n",
      "[3000]\ttraining's l2: 0.326591\tvalid_1's l2: 0.31686\n",
      "[3100]\ttraining's l2: 0.325529\tvalid_1's l2: 0.315902\n",
      "[3200]\ttraining's l2: 0.324481\tvalid_1's l2: 0.314949\n",
      "[3300]\ttraining's l2: 0.323472\tvalid_1's l2: 0.314038\n",
      "[3400]\ttraining's l2: 0.322446\tvalid_1's l2: 0.313081\n",
      "[3500]\ttraining's l2: 0.321433\tvalid_1's l2: 0.312124\n",
      "[3600]\ttraining's l2: 0.320434\tvalid_1's l2: 0.311197\n",
      "[3700]\ttraining's l2: 0.319453\tvalid_1's l2: 0.310263\n",
      "[3800]\ttraining's l2: 0.318485\tvalid_1's l2: 0.309382\n",
      "[3900]\ttraining's l2: 0.317536\tvalid_1's l2: 0.308514\n",
      "[4000]\ttraining's l2: 0.316605\tvalid_1's l2: 0.307657\n",
      "[4100]\ttraining's l2: 0.315648\tvalid_1's l2: 0.306772\n",
      "[4200]\ttraining's l2: 0.314704\tvalid_1's l2: 0.305899\n",
      "[4300]\ttraining's l2: 0.313781\tvalid_1's l2: 0.305039\n",
      "[4400]\ttraining's l2: 0.31289\tvalid_1's l2: 0.304232\n",
      "[4500]\ttraining's l2: 0.311992\tvalid_1's l2: 0.303387\n",
      "[4600]\ttraining's l2: 0.311057\tvalid_1's l2: 0.302518\n",
      "[4700]\ttraining's l2: 0.310151\tvalid_1's l2: 0.30166\n",
      "[4800]\ttraining's l2: 0.309256\tvalid_1's l2: 0.300831\n",
      "[4900]\ttraining's l2: 0.308394\tvalid_1's l2: 0.299988\n",
      "[5000]\ttraining's l2: 0.307524\tvalid_1's l2: 0.299186\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l2: 0.307524\tvalid_1's l2: 0.299186\n",
      "mean_21: 4483534.12\n",
      "mean_30: 3890582.73\n",
      "mean_16: 2197163.20\n",
      "mean_20_dow6_2017: 651669.31\n",
      "promo_13: 627281.30\n",
      "mean_60: 396238.67\n",
      "mean_7: 353331.84\n",
      "mean_4_dow6_2017: 333773.60\n",
      "median_60: 262854.43\n",
      "mean_3: 196884.05\n",
      "median_16: 169623.09\n",
      "std_140: 107119.59\n",
      "day_1_2017: 76968.52\n",
      "mean_20_dow5_2017: 75799.17\n",
      "std_60: 74463.11\n",
      "mean_140: 70694.93\n",
      "median_3: 64805.59\n",
      "mean_20_dow1_2017: 63127.57\n",
      "std_30: 63028.29\n",
      "mean_4_dow5_2017: 60147.88\n",
      "std_21: 59486.14\n",
      "mean_14: 57500.86\n",
      "promo_140: 55743.99\n",
      "median_140: 55281.00\n",
      "mean_4_dow1_2017: 54233.42\n",
      "mean_20_dow0_2017: 54045.74\n",
      "std_7: 53613.68\n",
      "mean_4_dow3_2017: 52975.67\n",
      "mean_4_dow4_2017: 52826.86\n",
      "mean_4_dow0_2017: 52510.97\n",
      "std_3: 50451.36\n",
      "mean_4_dow2_2017: 49924.72\n",
      "mean_20_dow2_2017: 48228.80\n",
      "promo_14: 48148.72\n",
      "mean_20_dow4_2017: 47898.89\n",
      "mean_20_dow3_2017: 46564.64\n",
      "std_16: 46541.85\n",
      "promo_16: 45724.51\n",
      "std_14: 42777.72\n",
      "promo_60: 39039.66\n",
      "promo_30: 36956.93\n",
      "promo_21: 36592.31\n",
      "median_7: 35769.22\n",
      "promo_12: 33333.33\n",
      "median_30: 30815.83\n",
      "promo_10: 29429.84\n",
      "median_14: 29312.93\n",
      "promo_6: 22556.24\n",
      "median_21: 21776.42\n",
      "max_3: 15540.24\n",
      "promo_0: 13761.87\n",
      "promo_7: 13017.12\n",
      "promo_15: 12646.40\n",
      "promo_11: 11954.85\n",
      "promo_9: 9492.96\n",
      "promo_8: 9125.46\n",
      "max_7: 6011.91\n",
      "promo_1: 4134.29\n",
      "promo_2: 3994.08\n",
      "promo_4: 3951.56\n",
      "max_14: 3456.02\n",
      "promo_5: 2893.41\n",
      "promo_3: 2433.07\n",
      "min_7: 1902.17\n",
      "min_3: 1855.41\n",
      "max_140: 1541.60\n",
      "max_16: 1062.63\n",
      "max_30: 954.89\n",
      "max_60: 886.19\n",
      "max_21: 458.20\n",
      "min_14: 323.60\n",
      "min_16: 57.62\n",
      "min_21: 12.54\n",
      "min_30: 10.23\n",
      "min_60: 0.00\n",
      "min_140: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Step 15\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's l2: 0.375962\tvalid_1's l2: 0.362253\n",
      "[200]\ttraining's l2: 0.35267\tvalid_1's l2: 0.341906\n",
      "[300]\ttraining's l2: 0.347558\tvalid_1's l2: 0.337547\n",
      "[400]\ttraining's l2: 0.344549\tvalid_1's l2: 0.334917\n",
      "[500]\ttraining's l2: 0.342326\tvalid_1's l2: 0.332988\n",
      "[600]\ttraining's l2: 0.340427\tvalid_1's l2: 0.331335\n",
      "[700]\ttraining's l2: 0.338704\tvalid_1's l2: 0.329806\n",
      "[800]\ttraining's l2: 0.337009\tvalid_1's l2: 0.328286\n",
      "[900]\ttraining's l2: 0.33543\tvalid_1's l2: 0.326887\n",
      "[1000]\ttraining's l2: 0.333913\tvalid_1's l2: 0.325531\n",
      "[1100]\ttraining's l2: 0.332454\tvalid_1's l2: 0.324266\n",
      "[1200]\ttraining's l2: 0.33108\tvalid_1's l2: 0.323031\n",
      "[1300]\ttraining's l2: 0.329733\tvalid_1's l2: 0.321829\n",
      "[1400]\ttraining's l2: 0.328409\tvalid_1's l2: 0.320643\n",
      "[1500]\ttraining's l2: 0.327144\tvalid_1's l2: 0.31949\n",
      "[1600]\ttraining's l2: 0.325904\tvalid_1's l2: 0.318334\n",
      "[1700]\ttraining's l2: 0.324685\tvalid_1's l2: 0.317225\n",
      "[1800]\ttraining's l2: 0.323486\tvalid_1's l2: 0.316122\n",
      "[1900]\ttraining's l2: 0.32233\tvalid_1's l2: 0.315106\n",
      "[2000]\ttraining's l2: 0.321154\tvalid_1's l2: 0.313995\n",
      "[2100]\ttraining's l2: 0.32002\tvalid_1's l2: 0.312959\n",
      "[2200]\ttraining's l2: 0.318892\tvalid_1's l2: 0.311913\n",
      "[2300]\ttraining's l2: 0.317786\tvalid_1's l2: 0.310912\n",
      "[2400]\ttraining's l2: 0.316702\tvalid_1's l2: 0.309929\n",
      "[2500]\ttraining's l2: 0.315668\tvalid_1's l2: 0.308981\n",
      "[2600]\ttraining's l2: 0.314613\tvalid_1's l2: 0.307991\n",
      "[2700]\ttraining's l2: 0.313584\tvalid_1's l2: 0.307009\n",
      "[2800]\ttraining's l2: 0.312568\tvalid_1's l2: 0.306078\n",
      "[2900]\ttraining's l2: 0.311551\tvalid_1's l2: 0.305154\n",
      "[3000]\ttraining's l2: 0.310552\tvalid_1's l2: 0.304228\n",
      "[3100]\ttraining's l2: 0.309583\tvalid_1's l2: 0.303311\n",
      "[3200]\ttraining's l2: 0.308623\tvalid_1's l2: 0.302443\n",
      "[3300]\ttraining's l2: 0.307655\tvalid_1's l2: 0.30155\n",
      "[3400]\ttraining's l2: 0.306723\tvalid_1's l2: 0.300672\n",
      "[3500]\ttraining's l2: 0.305772\tvalid_1's l2: 0.299807\n",
      "[3600]\ttraining's l2: 0.30484\tvalid_1's l2: 0.298936\n",
      "[3700]\ttraining's l2: 0.3039\tvalid_1's l2: 0.298056\n",
      "[3800]\ttraining's l2: 0.302982\tvalid_1's l2: 0.297189\n",
      "[3900]\ttraining's l2: 0.302062\tvalid_1's l2: 0.296353\n",
      "[4000]\ttraining's l2: 0.301159\tvalid_1's l2: 0.29551\n",
      "[4100]\ttraining's l2: 0.300288\tvalid_1's l2: 0.294717\n",
      "[4200]\ttraining's l2: 0.299412\tvalid_1's l2: 0.293901\n",
      "[4300]\ttraining's l2: 0.298557\tvalid_1's l2: 0.2931\n",
      "[4400]\ttraining's l2: 0.297693\tvalid_1's l2: 0.292283\n",
      "[4500]\ttraining's l2: 0.296844\tvalid_1's l2: 0.291477\n",
      "[4600]\ttraining's l2: 0.295984\tvalid_1's l2: 0.290666\n",
      "[4700]\ttraining's l2: 0.295149\tvalid_1's l2: 0.289861\n",
      "[4800]\ttraining's l2: 0.294329\tvalid_1's l2: 0.289106\n",
      "[4900]\ttraining's l2: 0.293508\tvalid_1's l2: 0.288325\n",
      "[5000]\ttraining's l2: 0.292676\tvalid_1's l2: 0.287536\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l2: 0.292676\tvalid_1's l2: 0.287536\n",
      "mean_21: 6027404.62\n",
      "mean_30: 3059084.20\n",
      "median_60: 914551.99\n",
      "mean_16: 910555.89\n",
      "mean_7: 789552.40\n",
      "promo_14: 759113.47\n",
      "mean_20_dow0_2017: 686049.42\n",
      "mean_14: 411478.06\n",
      "mean_4_dow0_2017: 225136.25\n",
      "mean_60: 222738.83\n",
      "std_140: 131509.92\n",
      "mean_3: 79799.04\n",
      "median_7: 75298.65\n",
      "std_60: 72204.75\n",
      "mean_20_dow2_2017: 71545.37\n",
      "std_30: 65915.48\n",
      "promo_140: 64495.83\n",
      "promo_7: 63771.68\n",
      "mean_140: 61511.44\n",
      "promo_0: 58230.77\n",
      "std_21: 56224.70\n",
      "mean_4_dow2_2017: 53051.98\n",
      "mean_4_dow5_2017: 52705.42\n",
      "mean_20_dow4_2017: 52448.29\n",
      "day_1_2017: 50913.45\n",
      "mean_20_dow1_2017: 50784.27\n",
      "mean_4_dow6_2017: 50110.37\n",
      "std_7: 49767.28\n",
      "mean_4_dow1_2017: 49432.08\n",
      "mean_4_dow4_2017: 49255.56\n",
      "mean_4_dow3_2017: 49043.55\n",
      "median_16: 48856.95\n",
      "mean_20_dow6_2017: 47991.12\n",
      "std_3: 46701.88\n",
      "mean_20_dow5_2017: 45687.61\n",
      "promo_60: 44245.16\n",
      "mean_20_dow3_2017: 43540.30\n",
      "median_140: 41426.73\n",
      "promo_30: 41401.46\n",
      "std_16: 41371.50\n",
      "std_14: 40913.11\n",
      "median_3: 38549.92\n",
      "promo_16: 37793.90\n",
      "promo_13: 35058.12\n",
      "promo_21: 34396.06\n",
      "median_30: 32271.19\n",
      "promo_15: 28261.00\n",
      "median_14: 26348.26\n",
      "promo_12: 25734.23\n",
      "max_3: 25340.33\n",
      "promo_10: 21014.45\n",
      "median_21: 21011.40\n",
      "promo_9: 17139.31\n",
      "max_14: 12608.89\n",
      "max_7: 9980.19\n",
      "promo_8: 7627.52\n",
      "promo_11: 7140.38\n",
      "promo_2: 6645.57\n",
      "promo_6: 5719.10\n",
      "promo_4: 3859.70\n",
      "promo_1: 3318.18\n",
      "max_30: 2607.67\n",
      "promo_3: 2235.28\n",
      "max_21: 2193.73\n",
      "max_140: 2127.95\n",
      "min_7: 2000.71\n",
      "promo_5: 1820.07\n",
      "max_16: 1641.20\n",
      "max_60: 1367.48\n",
      "min_3: 1257.45\n",
      "min_14: 586.42\n",
      "min_21: 29.46\n",
      "min_60: 18.77\n",
      "min_30: 15.78\n",
      "min_16: 11.80\n",
      "min_140: 0.00\n",
      "==================================================\n",
      "Step 16\n",
      "==================================================\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's l2: 0.383497\tvalid_1's l2: 0.383099\n",
      "[200]\ttraining's l2: 0.363818\tvalid_1's l2: 0.364262\n",
      "[300]\ttraining's l2: 0.359034\tvalid_1's l2: 0.359893\n",
      "[400]\ttraining's l2: 0.355959\tvalid_1's l2: 0.357159\n",
      "[500]\ttraining's l2: 0.353668\tvalid_1's l2: 0.355056\n",
      "[600]\ttraining's l2: 0.351735\tvalid_1's l2: 0.353211\n",
      "[700]\ttraining's l2: 0.349955\tvalid_1's l2: 0.351565\n",
      "[800]\ttraining's l2: 0.348263\tvalid_1's l2: 0.34996\n",
      "[900]\ttraining's l2: 0.346623\tvalid_1's l2: 0.348433\n",
      "[1000]\ttraining's l2: 0.345069\tvalid_1's l2: 0.346934\n",
      "[1100]\ttraining's l2: 0.343581\tvalid_1's l2: 0.345528\n",
      "[1200]\ttraining's l2: 0.342127\tvalid_1's l2: 0.344165\n",
      "[1300]\ttraining's l2: 0.340746\tvalid_1's l2: 0.342875\n",
      "[1400]\ttraining's l2: 0.33936\tvalid_1's l2: 0.34154\n",
      "[1500]\ttraining's l2: 0.338025\tvalid_1's l2: 0.340279\n",
      "[1600]\ttraining's l2: 0.33672\tvalid_1's l2: 0.339053\n",
      "[1700]\ttraining's l2: 0.335417\tvalid_1's l2: 0.337804\n",
      "[1800]\ttraining's l2: 0.334171\tvalid_1's l2: 0.336613\n",
      "[1900]\ttraining's l2: 0.332954\tvalid_1's l2: 0.335467\n",
      "[2000]\ttraining's l2: 0.331758\tvalid_1's l2: 0.3343\n",
      "[2100]\ttraining's l2: 0.330575\tvalid_1's l2: 0.333158\n",
      "[2200]\ttraining's l2: 0.329389\tvalid_1's l2: 0.331994\n",
      "[2300]\ttraining's l2: 0.328235\tvalid_1's l2: 0.330901\n",
      "[2400]\ttraining's l2: 0.327133\tvalid_1's l2: 0.329832\n",
      "[2500]\ttraining's l2: 0.326004\tvalid_1's l2: 0.328703\n",
      "[2600]\ttraining's l2: 0.324918\tvalid_1's l2: 0.32769\n",
      "[2700]\ttraining's l2: 0.32383\tvalid_1's l2: 0.326608\n",
      "[2800]\ttraining's l2: 0.322747\tvalid_1's l2: 0.325604\n",
      "[2900]\ttraining's l2: 0.321712\tvalid_1's l2: 0.324608\n",
      "[3000]\ttraining's l2: 0.320683\tvalid_1's l2: 0.323637\n",
      "[3100]\ttraining's l2: 0.319664\tvalid_1's l2: 0.322643\n",
      "[3200]\ttraining's l2: 0.318646\tvalid_1's l2: 0.321643\n",
      "[3300]\ttraining's l2: 0.31768\tvalid_1's l2: 0.320688\n",
      "[3400]\ttraining's l2: 0.316676\tvalid_1's l2: 0.319711\n",
      "[3500]\ttraining's l2: 0.31568\tvalid_1's l2: 0.318739\n",
      "[3600]\ttraining's l2: 0.314714\tvalid_1's l2: 0.317801\n",
      "[3700]\ttraining's l2: 0.313754\tvalid_1's l2: 0.316878\n",
      "[3800]\ttraining's l2: 0.312813\tvalid_1's l2: 0.315954\n",
      "[3900]\ttraining's l2: 0.311864\tvalid_1's l2: 0.315004\n",
      "[4000]\ttraining's l2: 0.31092\tvalid_1's l2: 0.314061\n",
      "[4100]\ttraining's l2: 0.310005\tvalid_1's l2: 0.313153\n",
      "[4200]\ttraining's l2: 0.309077\tvalid_1's l2: 0.31225\n",
      "[4300]\ttraining's l2: 0.308176\tvalid_1's l2: 0.311383\n",
      "[4400]\ttraining's l2: 0.307277\tvalid_1's l2: 0.310509\n",
      "[4500]\ttraining's l2: 0.306394\tvalid_1's l2: 0.309649\n",
      "[4600]\ttraining's l2: 0.305527\tvalid_1's l2: 0.308796\n",
      "[4700]\ttraining's l2: 0.304681\tvalid_1's l2: 0.307966\n",
      "[4800]\ttraining's l2: 0.303825\tvalid_1's l2: 0.307113\n",
      "[4900]\ttraining's l2: 0.30296\tvalid_1's l2: 0.306274\n",
      "[5000]\ttraining's l2: 0.302115\tvalid_1's l2: 0.30546\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l2: 0.302115\tvalid_1's l2: 0.30546\n",
      "mean_21: 4251544.07\n",
      "mean_30: 3693160.95\n",
      "median_60: 902735.56\n",
      "promo_15: 525719.05\n",
      "mean_16: 516066.28\n",
      "mean_14: 490043.07\n",
      "mean_60: 436468.12\n",
      "mean_20_dow1_2017: 436290.36\n",
      "mean_7: 372773.58\n",
      "median_30: 207299.15\n",
      "std_140: 119503.59\n",
      "mean_20_dow2_2017: 109529.41\n",
      "mean_4_dow1_2017: 94784.02\n",
      "median_16: 78648.25\n",
      "std_60: 78457.38\n",
      "mean_3: 68566.75\n",
      "mean_140: 67357.66\n",
      "promo_140: 63918.12\n",
      "std_30: 63014.07\n",
      "promo_14: 61025.08\n",
      "mean_4_dow2_2017: 58434.26\n",
      "mean_20_dow4_2017: 56243.77\n",
      "std_7: 55180.65\n",
      "mean_4_dow6_2017: 53285.48\n",
      "median_140: 53009.34\n",
      "mean_4_dow0_2017: 52785.85\n",
      "mean_20_dow0_2017: 52323.53\n",
      "mean_4_dow5_2017: 51926.89\n",
      "mean_4_dow3_2017: 51901.75\n",
      "mean_4_dow4_2017: 51612.55\n",
      "std_21: 50810.45\n",
      "day_1_2017: 48942.17\n",
      "std_3: 48895.51\n",
      "mean_20_dow5_2017: 46609.79\n",
      "mean_20_dow6_2017: 45918.91\n",
      "mean_20_dow3_2017: 44368.65\n",
      "std_14: 41273.77\n",
      "std_16: 40432.82\n",
      "median_7: 38420.42\n",
      "promo_60: 38336.75\n",
      "promo_30: 37781.69\n",
      "median_14: 36319.80\n",
      "promo_21: 33717.94\n",
      "median_3: 32856.29\n",
      "median_21: 26543.23\n",
      "promo_16: 24071.28\n",
      "promo_10: 14302.44\n",
      "promo_13: 14152.47\n",
      "promo_12: 12278.21\n",
      "promo_8: 12116.05\n",
      "promo_0: 12082.52\n",
      "promo_7: 11384.10\n",
      "max_3: 7420.75\n",
      "promo_9: 7027.48\n",
      "max_7: 6676.76\n",
      "max_14: 5586.56\n",
      "promo_11: 4995.30\n",
      "promo_1: 3907.09\n",
      "promo_6: 3781.29\n",
      "promo_2: 3736.05\n",
      "promo_4: 3113.53\n",
      "promo_3: 2415.94\n",
      "min_3: 2361.65\n",
      "max_30: 1901.62\n",
      "max_16: 1780.71\n",
      "max_140: 1551.98\n",
      "promo_5: 1461.85\n",
      "max_60: 1029.09\n",
      "max_21: 917.29\n",
      "min_7: 552.27\n",
      "min_14: 173.42\n",
      "min_21: 69.62\n",
      "min_16: 21.26\n",
      "min_30: 19.31\n",
      "min_60: 2.65\n",
      "min_140: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mse: 0.2959389420983095\n"
     ]
    }
   ],
   "source": [
    "print(\"Training and predicting models...\")\n",
    "params = {\n",
    "    'num_leaves': 80,\n",
    "    'objective': 'regression',\n",
    "    'min_data_in_leaf': 200,\n",
    "    'learning_rate': 0.02,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 1,\n",
    "    'metric': 'l2',\n",
    "    'num_threads': 16\n",
    "}\n",
    "\n",
    "MAX_ROUNDS = 5000\n",
    "val_pred = []\n",
    "test_pred = []\n",
    "cate_vars = []\n",
    "for i in range(16):\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Step %d\" % (i+1))\n",
    "    print(\"=\" * 50)\n",
    "    dtrain = lgb.Dataset(\n",
    "        X_train, label=y_train[:, i],\n",
    "        categorical_feature=cate_vars,\n",
    "        weight=pd.concat([items[\"perishable\"]] * 4) * 0.25 + 1\n",
    "    )\n",
    "    \n",
    "    dval = lgb.Dataset(\n",
    "        X_val, label=y_val[:, i], reference=dtrain,\n",
    "        categorical_feature=cate_vars,\n",
    "        weight=items[\"perishable\"] * 0.25 + 1\n",
    "    )\n",
    "    \n",
    "    bst = lgb.train(\n",
    "        params, dtrain, num_boost_round=MAX_ROUNDS,\n",
    "        valid_sets=[dtrain, dval], early_stopping_rounds=50, verbose_eval=100\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\".join((\"%s: %.2f\" % x) for x in sorted(\n",
    "        zip(X_train.columns, bst.feature_importance(\"gain\")),\n",
    "        key=lambda x: x[1], reverse=True\n",
    "    )))\n",
    "    val_pred.append(bst.predict(\n",
    "        X_val, num_iteration=bst.best_iteration or MAX_ROUNDS))\n",
    "    test_pred.append(bst.predict(\n",
    "        X_test, num_iteration=bst.best_iteration or MAX_ROUNDS))\n",
    "\n",
    "print(\"Validation mse:\", mean_squared_error(\n",
    "    y_val, np.array(val_pred).transpose()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 167515)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_pred).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making submission...\n"
     ]
    }
   ],
   "source": [
    "print(\"Making submission...\")\n",
    "y_test = np.array(test_pred).transpose()\n",
    "df_preds = pd.DataFrame(\n",
    "    y_test, index=df_train.index,\n",
    "    columns=pd.date_range(\"2017-08-16\", periods=16)\n",
    ").stack().to_frame(\"unit_sales\")\n",
    "df_preds.index.set_names([\"store_nbr\", \"item_nbr\", \"date\"], inplace=True)\n",
    "\n",
    "submission = df_test[[\"id\"]].join(df_preds, how=\"left\").fillna(0)\n",
    "submission[\"unit_sales\"] = np.clip(np.expm1(submission[\"unit_sales\"]), 0, 1000)\n",
    "submission.to_csv('../Result/modified_weight_promo_year_1.csv', float_format='%.4f', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167515, 16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
